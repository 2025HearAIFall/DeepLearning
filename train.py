# train.py (ìˆ˜ì •: Seq2Seq í•™ìŠµ)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torch.optim.lr_scheduler import ReduceLROnPlateau
from tqdm import tqdm
import pandas as pd
import numpy as np
import math
from collections import Counter

# ğŸ’¡ [ìˆ˜ì •] data_loader, model ì„í¬íŠ¸
from data_loader import SignLanguageDataset
from model import Encoder, Decoder, Seq2Seq

# --- í† í¬ë‚˜ì´ì € ë° Vocabulary í´ë˜ìŠ¤ ì •ì˜ ---
# ğŸ’¡ (ê°„ë‹¨í•œ ì˜ˆì‹œ: ê³µë°± ê¸°ì¤€ í† í¬ë‚˜ì´ì €. í˜•íƒœì†Œ ë¶„ì„ê¸°(Mecab) ì‚¬ìš© ê¶Œì¥)
def simple_tokenizer(text):
    return text.split(' ')

class Vocabulary:
    def __init__(self, tokenizer, min_freq=2):
        self.tokenizer = tokenizer
        
        # 0: <PAD>, 1: <SOS>, 2: <EOS>, 3: <UNK>
        self.itos = {0: "<PAD>", 1: "<SOS>", 2: "<EOS>", 3: "<UNK>"}
        self.stoi = {v: k for k, v in self.itos.items()}
        self.min_freq = min_freq
        
    def __len__(self):
        return len(self.itos)
    
    def build_vocab(self, sentence_list):
        counter = Counter()
        for sentence in sentence_list:
            counter.update(self.tokenizer(sentence))
            
        idx = 4 # 0-3ì€ íŠ¹ìˆ˜ í† í°
        for word, freq in counter.items():
            if freq >= self.min_freq:
                self.stoi[word] = idx
                self.itos[idx] = word
                idx += 1
                
    def numericalize(self, text):
        tokens = self.tokenizer(text)
        return [self.stoi.get(token, self.stoi["<UNK>"]) for token in tokens]
    
    @property
    def pad_idx(self): return self.stoi["<PAD>"]
    @property
    def sos_idx(self): return self.stoi["<SOS>"]
    @property
    def eos_idx(self): return self.stoi["<EOS>"]
    @property
    def unk_idx(self): return self.stoi["<UNK>"]


# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 32       # ğŸ’¡ Seq2SeqëŠ” ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•˜ë¯€ë¡œ BATCH_SIZE ì¤„ì„
LEARNING_RATE = 0.001
NUM_EPOCHS = 100      # ğŸ’¡ ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•´ ì¦ê°€
MAX_LEN = 30          # ğŸ’¡ [ìœ ì§€] ì…ë ¥ í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´
MAX_TARGET_LEN = 50   # ğŸ’¡ [ì¶”ê°€] íƒ€ê²Ÿ ë¬¸ì¥ ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´
INPUT_SIZE = 548      # ğŸ’¡ [ìœ ì§€] (Pose+Face+Hands) * 2 (pos+mot) = 548
HIDDEN_SIZE = 512     # ğŸ’¡ [ìœ ì§€]
NUM_LAYERS = 3        # ğŸ’¡ [ìœ ì§€]
EMBED_SIZE = 256      # ğŸ’¡ [ì¶”ê°€] íƒ€ê²Ÿ ë‹¨ì–´ ì„ë² ë”© í¬ê¸°
DROPOUT_PROB = 0.5    # ğŸ’¡ [ìˆ˜ì •] Dropout ê°’ ëª…ì‹œ (ê¸°ì¡´ 0.6)
NUM_WORKERS = 0
PATIENCE = 10         # ğŸ’¡ [ìˆ˜ì •] ì¡°ê¸° ì¢…ë£Œ Patience ì¦ê°€

# --- ë°ì´í„° ì¤€ë¹„ ë° Vocabulary ìƒì„± ---
print("ì „ì²´ ë°ì´í„° ì¸ë±ìŠ¤ ë¡œë”© ë° Vocabulary ìƒì„±...")
train_df = pd.read_csv('training_index.csv')
valid_df = pd.read_csv('validation_index.csv')

all_df = pd.concat([train_df, valid_df], ignore_index=True)
all_sentences = all_df['sentence'].tolist() # ğŸ’¡ 'label' -> 'sentence'

# ğŸ’¡ Vocabulary ê°ì²´ ìƒì„± ë° ë¹Œë“œ
vocab = Vocabulary(simple_tokenizer, min_freq=2)
vocab.build_vocab(all_sentences)
TARGET_VOCAB_SIZE = len(vocab) # ğŸ’¡ [ì¶”ê°€]
PAD_IDX = vocab.pad_idx

print(f"ìƒì„±ëœ Target Vocabulary í¬ê¸°: {TARGET_VOCAB_SIZE}")

print("í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¡œë”© ì‹œì‘...")
full_train_dataset = SignLanguageDataset(
    index_file_path='training_index.csv', 
    max_len=MAX_LEN, 
    max_target_len=MAX_TARGET_LEN, # ğŸ’¡ ì¶”ê°€
    vocab=vocab                  # ğŸ’¡ ì£¼ì…
)
train_size = int(0.8 * len(full_train_dataset))
valid_size = len(full_train_dataset) - train_size
train_dataset, valid_dataset = random_split(full_train_dataset, [train_size, valid_size])

print("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì‹œì‘...")
test_dataset = SignLanguageDataset(
    index_file_path='validation_index.csv', 
    max_len=MAX_LEN, 
    max_target_len=MAX_TARGET_LEN, # ğŸ’¡ ì¶”ê°€
    vocab=vocab                  # ğŸ’¡ ì£¼ì…
)

# ğŸ’¡ [ì£¼ì˜] DataLoaderëŠ” ì´ì œ (keypoints, target_sentences) íŠœí”Œì„ ë°˜í™˜
train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)
valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)
test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)

print(f"ë¶„ë¦¬ëœ í•™ìŠµ ë°ì´í„° ìˆ˜: {len(train_dataset)}, ê²€ì¦ ë°ì´í„° ìˆ˜: {len(valid_dataset)}")
print(f"ë³„ë„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìˆ˜: {len(test_dataset)}")

# --- ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ ì •ì˜ ---
# ğŸ’¡ [ìˆ˜ì •] Seq2Seq ëª¨ë¸ ì •ì˜
encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT_PROB)
decoder = Decoder(TARGET_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT_PROB)
model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)

# ğŸ’¡ [ìˆ˜ì •] íŒ¨ë”© í† í°ì€ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸
criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)

best_val_loss = float('inf')
patience_counter = 0
best_model_path = 'sign_language_model_seq2seq_best.pth'

# --- ëª¨ë¸ í•™ìŠµ ì‹œì‘ ---
print(f"\nSeq2Seq ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤... (DEVICE: {DEVICE})")
for epoch in range(NUM_EPOCHS):
    model.train()
    train_loss = 0.0
    
    for (keypoints, targets) in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Training]"):
        keypoints, targets = keypoints.to(DEVICE), targets.to(DEVICE)
        
        # keypoints: (batch, max_len, 548)
        # targets: (batch, max_target_len)
        
        optimizer.zero_grad()
        
        # ğŸ’¡ [ìˆ˜ì •] Seq2Seq ëª¨ë¸ forward
        # outputs: (batch, max_target_len, vocab_size)
        outputs = model(keypoints, targets, teacher_forcing_ratio=0.5)
        
        # ğŸ’¡ [ìˆ˜ì •] ì†ì‹¤ ê³„ì‚° (CrossEntropyLossëŠ” 2D ì…ë ¥ì„ ê¸°ëŒ€)
        # outputs: (batch * (max_target_len-1), vocab_size)
        output_dim = outputs.shape[-1]
        outputs_flat = outputs[:, 1:, :].reshape(-1, output_dim) # <SOS> ì œì™¸
        
        # targets: (batch * (max_target_len-1))
        targets_flat = targets[:, 1:].reshape(-1) # <SOS> ì œì™¸
        
        loss = criterion(outputs_flat, targets_flat)
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        train_loss += loss.item()

    # --- ê²€ì¦ ---
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for (keypoints, targets) in tqdm(valid_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Validation]"):
            keypoints, targets = keypoints.to(DEVICE), targets.to(DEVICE)
            
            # ğŸ’¡ [ìˆ˜ì •] ê²€ì¦ ì‹œì—ëŠ” Teacher Forcing ë” (ratio=0.0)
            outputs = model(keypoints, targets, teacher_forcing_ratio=0.0)
            
            output_dim = outputs.shape[-1]
            outputs_flat = outputs[:, 1:, :].reshape(-1, output_dim)
            targets_flat = targets[:, 1:].reshape(-1)
            
            loss = criterion(outputs_flat, targets_flat)
            val_loss += loss.item()

    avg_train_loss = train_loss / len(train_loader)
    avg_val_loss = val_loss / len(valid_loader)
    
    # ğŸ’¡ [ìˆ˜ì •] ë¶„ë¥˜ ì •í™•ë„ ëŒ€ì‹  Perplexity (PPL) ì¶œë ¥
    try:
        train_ppl = math.exp(avg_train_loss)
        val_ppl = math.exp(avg_val_loss)
    except OverflowError:
        train_ppl = float('inf')
        val_ppl = float('inf')

    print(f"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {avg_train_loss:.4f} (PPL: {train_ppl:.2f}), Val Loss: {avg_val_loss:.4f} (PPL: {val_ppl:.2f})")
    
    scheduler.step(avg_val_loss)

    # ğŸ’¡ [ìˆ˜ì •] ì¡°ê¸° ì¢…ë£Œ ë¡œì§
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        patience_counter = 0
        torch.save(model.state_dict(), best_model_path)
        print(f"ğŸ‰ New best model saved with Val Loss: {best_val_loss:.4f}")
    else:
        patience_counter += 1
        print(f"Patience: {patience_counter}/{PATIENCE}")
    
    if patience_counter >= PATIENCE:
        print(f"Early stopping triggered after {epoch+1} epochs.")
        break

print("\ní•™ìŠµ ì™„ë£Œ!")

# --- ìµœì¢… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---
print(f"\nìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸({best_model_path})ì„ ë¡œë“œí•˜ì—¬ ìµœì¢… í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")
try:
    model.load_state_dict(torch.load(best_model_path))
except FileNotFoundError:
    print(f"[ê²½ê³ ] {best_model_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ epochì˜ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    
model.eval()
test_loss = 0.0
with torch.no_grad():
    for (keypoints, targets) in tqdm(test_loader, desc="Final Test"):
        keypoints, targets = keypoints.to(DEVICE), targets.to(DEVICE)
        outputs = model(keypoints, targets, teacher_forcing_ratio=0.0)
        
        output_dim = outputs.shape[-1]
        outputs_flat = outputs[:, 1:, :].reshape(-1, output_dim)
        targets_flat = targets[:, 1:].reshape(-1)
        
        loss = criterion(outputs_flat, targets_flat)
        test_loss += loss.item()

avg_test_loss = test_loss / len(test_loader)
test_ppl = math.exp(avg_test_loss)
print(f"\nìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Test Loss): {avg_test_loss:.4f}, í…ŒìŠ¤íŠ¸ Perplexity (PPL): {test_ppl:.2f}")

print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ '{best_model_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
# ğŸ’¡ Vocabularyë„ ì €ì¥í•´ì•¼ ì‹¤ì œ ì¶”ë¡ ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
import pickle
with open('vocab.pkl', 'wb') as f:
    pickle.dump(vocab, f)
print("Vocabularyê°€ 'vocab.pkl' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")