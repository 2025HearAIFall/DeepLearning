# train.py (ìˆ˜ì •: if __name__ == '__main__' ë¸”ë¡ ì¶”ê°€)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torch.optim.lr_scheduler import ReduceLROnPlateau
from tqdm import tqdm
import pandas as pd
import numpy as np
import math
from collections import Counter
import pickle
import os 

from data_loader import SignLanguageDataset
from model import Encoder, Decoder, Seq2Seq, Attention # ğŸ’¡ Attention ì„í¬íŠ¸

# --- í† í¬ë‚˜ì´ì € ë° Vocabulary í´ë˜ìŠ¤ ì •ì˜ ---
# (ì´ ë¶€ë¶„ì€ import ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ë°–ì— ë‘¡ë‹ˆë‹¤)
def simple_tokenizer(text):
    return text.split(' ')

class Vocabulary:
    def __init__(self, tokenizer, min_freq=2):
        self.tokenizer = tokenizer
        
        # 0: <PAD>, 1: <SOS>, 2: <EOS>, 3: <UNK>
        self.itos = {0: "<PAD>", 1: "<SOS>", 2: "<EOS>", 3: "<UNK>"}
        self.stoi = {v: k for k, v in self.itos.items()}
        self.min_freq = min_freq
        
    def __len__(self):
        return len(self.itos)
    
    def build_vocab(self, sentence_list):
        counter = Counter()
        for sentence in sentence_list:
            counter.update(self.tokenizer(sentence))
            
        idx = 4 
        for word, freq in counter.items():
            if freq >= self.min_freq:
                self.stoi[word] = idx
                self.itos[idx] = word
                idx += 1
                
    def numericalize(self, text):
        tokens = self.tokenizer(text)
        return [self.stoi.get(token, self.stoi["<UNK>"]) for token in tokens]
    
    @property
    def pad_idx(self): return self.stoi["<PAD>"]
    @property
    def sos_idx(self): return self.stoi["<SOS>"]
    @property
    def eos_idx(self): return self.stoi["<EOS>"]
    @property
    def unk_idx(self): return self.stoi["<UNK>"]

# --------------------------------------------------
# ğŸ’¡ [ìˆ˜ì •] ì—¬ê¸°ë¶€í„° íŒŒì¼ ëê¹Œì§€ ëª¨ë‘ if __name__ == '__main__': ë¸”ë¡ ì•ˆìœ¼ë¡œ ì´ë™
# --------------------------------------------------

if __name__ == '__main__':
    
    # --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ---
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    BATCH_SIZE = 32
    LEARNING_RATE = 0.0005  
    NUM_EPOCHS = 15 # ğŸ’¡ [ìˆ˜ì •] Epoch 15ë¡œ ì¡°ì • (ë¡œê·¸ ê¸°ì¤€)
    MAX_LEN = 30
    MAX_TARGET_LEN = 50
    INPUT_SIZE = 548
    HIDDEN_SIZE = 512
    NUM_LAYERS = 3
    EMBED_SIZE = 256
    DROPOUT_PROB = 0.6
    NUM_WORKERS = 0
    PATIENCE = 10 # ğŸ’¡ [ìˆ˜ì •] Patience 10ìœ¼ë¡œ ì¡°ì •

    # --- ë°ì´í„° ì¤€ë¹„ ë° Vocabulary ìƒì„± ---
    print("ì „ì²´ ë°ì´í„° ì¸ë±ìŠ¤ ë¡œë”© ë° Vocabulary ìƒì„±...")
    try:
        train_df = pd.read_csv('training_index.csv')
        valid_df = pd.read_csv('validation_index.csv')
    except FileNotFoundError:
        print("[ì˜¤ë¥˜] training_index.csv ë˜ëŠ” validation_index.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    all_df = pd.concat([train_df, valid_df], ignore_index=True)
    all_sentences = all_df['sentence'].tolist()

    vocab = Vocabulary(simple_tokenizer, min_freq=2)
    vocab.build_vocab(all_sentences)
    TARGET_VOCAB_SIZE = len(vocab)
    PAD_IDX = vocab.pad_idx

    print(f"ìƒì„±ëœ Target Vocabulary í¬ê¸°: {TARGET_VOCAB_SIZE}")

    print("í†µí•© ë°ì´í„°ì…‹ ìƒì„± ë° ë¶„í• ...")

    COMBINED_INDEX_FILE = 'combined_index.csv'
    all_df.to_csv(COMBINED_INDEX_FILE, index=False, encoding='utf-8-sig')

    try:
        full_dataset = SignLanguageDataset(
            index_file_path=COMBINED_INDEX_FILE, 
            max_len=MAX_LEN, 
            max_target_len=MAX_TARGET_LEN,
            vocab=vocab
        )
    except Exception as e:
        print(f"ë°ì´í„°ì…‹ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        exit()

    if os.path.exists(COMBINED_INDEX_FILE):
        os.remove(COMBINED_INDEX_FILE)

    total_size = len(full_dataset)
    train_size = int(total_size * 0.8)
    valid_size = int(total_size * 0.1)
    test_size = total_size - train_size - valid_size 

    generator = torch.Generator().manual_seed(42)
    train_dataset, valid_dataset, test_dataset = random_split(
        full_dataset, [train_size, valid_size, test_size], generator=generator
    )

    print(f"ì´ ë°ì´í„°: {total_size}ê°œ")
    print(f"ë¶„ë¦¬ëœ í•™ìŠµ ë°ì´í„° ìˆ˜: {len(train_dataset)} (80%)")
    print(f"ë¶„ë¦¬ëœ ê²€ì¦ ë°ì´í„° ìˆ˜: {len(valid_dataset)} (10%)")
    print(f"ë¶„ë¦¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìˆ˜: {len(test_dataset)} (10%)")

    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)
    valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)
    test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)

    # --- ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ ì •ì˜ ---
    encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT_PROB)
    decoder = Decoder(TARGET_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT_PROB)
    model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)

    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)

    best_val_loss = float('inf')
    patience_counter = 0
    best_model_path = 'sign_language_model_seq2seq_best.pth'

    # --- ëª¨ë¸ í•™ìŠµ ì‹œì‘ ---
    print(f"\nSeq2Seq ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤... (DEVICE: {DEVICE})")
    for epoch in range(NUM_EPOCHS):
        model.train()
        train_loss = 0.0
        
        for (keypoints, targets) in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Training]"):
            keypoints, targets = keypoints.to(DEVICE), targets.to(DEVICE)
            
            optimizer.zero_grad()
            
            outputs = model(keypoints, targets, teacher_forcing_ratio=0.5)
            
            output_dim = outputs.shape[-1]
            outputs_flat = outputs[:, 1:, :].reshape(-1, output_dim)
            targets_flat = targets[:, 1:].reshape(-1)
            
            loss = criterion(outputs_flat, targets_flat)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()

        # --- ê²€ì¦ ---
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for (keypoints, targets) in tqdm(valid_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Validation]"):
                keypoints, targets = keypoints.to(DEVICE), targets.to(DEVICE)
                
                outputs = model(keypoints, targets, teacher_forcing_ratio=0.0)
                
                output_dim = outputs.shape[-1]
                outputs_flat = outputs[:, 1:, :].reshape(-1, output_dim)
                targets_flat = targets[:, 1:].reshape(-1)
                
                loss = criterion(outputs_flat, targets_flat)
                val_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(valid_loader)
        
        try:
            train_ppl = math.exp(avg_train_loss)
            val_ppl = math.exp(avg_val_loss)
        except OverflowError:
            train_ppl = float('inf')
            val_ppl = float('inf')

        print(f"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {avg_train_loss:.4f} (PPL: {train_ppl:.2f}), Val Loss: {avg_val_loss:.4f} (PPL: {val_ppl:.2f})")
        
        scheduler.step(avg_val_loss)

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            torch.save(model.state_dict(), best_model_path)
            print(f"ğŸ‰ New best model saved with Val Loss: {best_val_loss:.4f}")
        else:
            patience_counter += 1
            print(f"Patience: {patience_counter}/{PATIENCE}")
        
        if patience_counter >= PATIENCE:
            print(f"Early stopping triggered after {epoch+1} epochs.")
            break

    print("\ní•™ìŠµ ì™„ë£Œ!")

    # --- ìµœì¢… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---
    print(f"\nìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸({best_model_path})ì„ ë¡œë“œí•˜ì—¬ ìµœì¢… í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")
    try:
        model.load_state_dict(torch.load(best_model_path, weights_only=True))
    except FileNotFoundError:
        print(f"[ê²½ê³ ] {best_model_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ epochì˜ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"[ê²½ê³ ] ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ë§ˆì§€ë§‰ epochì˜ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
        
    model.eval()
    test_loss = 0.0
    with torch.no_grad():
        for (keypoints, targets) in tqdm(test_loader, desc="Final Test"):
            keypoints, targets = keypoints.to(DEVICE), targets.to(DEVICE)
            outputs = model(keypoints, targets, teacher_forcing_ratio=0.0)
            
            output_dim = outputs.shape[-1]
            outputs_flat = outputs[:, 1:, :].reshape(-1, output_dim)
            targets_flat = targets[:, 1:].reshape(-1)
            
            loss = criterion(outputs_flat, targets_flat)
            test_loss += loss.item()

    avg_test_loss = test_loss / len(test_loader)
    try:
        test_ppl = math.exp(avg_test_loss)
    except OverflowError:
        test_ppl = float('inf')
        
    print(f"\nìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Test Loss): {avg_test_loss:.4f}, í…ŒìŠ¤íŠ¸ Perplexity (PPL): {test_ppl:.2f}")

    print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ '{best_model_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    with open('vocab.pkl', 'wb') as f:
        pickle.dump(vocab, f)
    print("Vocabularyê°€ 'vocab.pkl' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
