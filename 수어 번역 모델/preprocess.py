# preprocess.py (ìµœì¢…: ë‹¨ì¼ ë‹¨ì–´(ë¬¸ë²• ì†ì„± í¬í•¨)ë§Œ ì¶”ì¶œ / ê²½ë¡œ ìë™ íƒìƒ‰ / íƒìƒ‰ ì§„í–‰ ìƒí™© ê²Œì´ì§€ ì¶”ê°€)

import os
import csv
from tqdm import tqdm
import re
from collections import defaultdict
import glob
import json # ğŸ’¡ JSON íŒŒì¼ì„ ì½ê¸° ìœ„í•´ import

sets_to_process = {
    'training': './videos/1.Training',
    'validation': './videos/2.Validation'
}

def _build_label_map(root_dir):
    """
    Morpheme JSON íŒŒì¼ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬
    'base_name'ì„ í‚¤ë¡œ, 'ë‹¨ì¼ ë‹¨ì–´ ë¼ë²¨'ì„ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    (ë¬¸ë²• ì†ì„±(attribute)ì´ ìˆì„ ê²½ìš° ë¼ë²¨ì— í¬í•¨í•©ë‹ˆë‹¤.)
    """
    print(f"-> '{root_dir}'ì—ì„œ ë¼ë²¨(morpheme) íŒŒì¼ ìŠ¤ìº” ì¤‘...")
    search_pattern = os.path.join(root_dir, "**", "*_morpheme.json")
    
    # [1/2] Morpheme íŒŒì¼ íƒìƒ‰ (ê²Œì´ì§€ í‘œì‹œ)
    morpheme_files_iter = glob.iglob(search_pattern, recursive=True)
    morpheme_files = list(tqdm(morpheme_files_iter, desc="[1/2] Morpheme íŒŒì¼ ì°¾ëŠ” ì¤‘"))
    
    label_map = {}
    skipped_multi_word = 0
    
    # [2/2] Morpheme íŒŒì¼ ì²˜ë¦¬ (ê²Œì´ì§€ í‘œì‹œ)
    for morpheme_path in tqdm(morpheme_files, desc="[2/2] Morpheme íŒŒì¼ ì²˜ë¦¬ ì¤‘"):
        try:
            base_name = os.path.basename(morpheme_path).replace('_morpheme.json', '')

            with open(morpheme_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            data_items = data.get('data')

            # ğŸ’¡ [í•µì‹¬ ë¡œì§]
            # 'data' í‚¤ê°€ ë¦¬ìŠ¤íŠ¸ì´ê³ , í•­ëª©ì´ 1ê°œì¼ ë•Œë§Œ (ë‹¨ì¼ ë‹¨ì–´)
            if isinstance(data_items, list) and len(data_items) == 1:
                item_attrs = data_items[0].get('attributes', [{}])[0]
                label_name = item_attrs.get('name') # e.g., "ì™¼ìª½"
                grammar_attr = item_attrs.get('attribute') # e.g., ["1í˜•íƒœì†Œ..."]

                if not label_name: # ë¼ë²¨ ì´ë¦„ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                    skipped_multi_word += 1
                    continue
                
                # ë¬¸ë²• ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸
                if isinstance(grammar_attr, list) and len(grammar_attr) > 0:
                    grammar_str = " ".join(grammar_attr)
                    final_label = f"{label_name} ({grammar_str})" # e.g., "ì™¼ìª½ (1í˜•íƒœì†Œ...)"
                else:
                    final_label = label_name # e.g., "ê°€ë½ë¡œ"
                
                label_map[base_name] = final_label
                    
            else:
                # í•­ëª©ì´ ì—†ê±°ë‚˜, 2ê°œ ì´ìƒì¸ 'ë¬¸ì¥' ë°ì´í„° (e.g., real_sen)
                skipped_multi_word += 1
        
        except Exception as e:
            skipped_multi_word += 1
            pass # ì˜¤ë¥˜ê°€ ë‚˜ë„ ê³„ì† ì§„í–‰
    
    print(f"-> ì´ {len(morpheme_files)}ê°œ morpheme íŒŒì¼ ë°œê²¬.")
    print(f"-> {len(label_map)}ê°œì˜ ë‹¨ì¼ ë‹¨ì–´ ë¼ë²¨(ë¬¸ë²• ì†ì„± í¬í•¨)ì„ ë§µí•‘í–ˆìŠµë‹ˆë‹¤. (ë¬¸ì¥/ì˜¤ë¥˜ {skipped_multi_word}ê°œ ìŠ¤í‚µ)")
    return label_map

def create_index_file_optimized(dataset_name, root_dir):
    print(f"\n'{dataset_name}' ë°ì´í„°ì…‹ ì¸ë±ìŠ¤ ìƒì„± (íŒŒì¼ ê²½ë¡œ ìë™ íƒìƒ‰)...")
    
    output_filename = f'{dataset_name}_index.csv'
    
    if not os.path.isdir(root_dir):
        print(f"-> [âŒ ì˜¤ë¥˜] ìµœìƒìœ„ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {root_dir}")
        return

    # 1. ë¼ë²¨ ë§µ ìƒì„±
    label_map = _build_label_map(root_dir)
    if not label_map:
        print(f"-> [âŒ ì˜¤ë¥˜] '{root_dir}'ì—ì„œ ìœ íš¨í•œ (ë‹¨ì¼ ë‹¨ì–´) morpheme íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"'{root_dir}' ê²½ë¡œ ë° ëª¨ë“  í•˜ìœ„ í´ë”ì—ì„œ keypoint íŒŒì¼ì„ íƒìƒ‰í•©ë‹ˆë‹¤...")
    search_pattern = os.path.join(root_dir, "**", "*_keypoints.json")
    
    # [1/3] Keypoint íŒŒì¼ íƒìƒ‰ (ê²Œì´ì§€ í‘œì‹œ)
    all_files_iter = glob.iglob(search_pattern, recursive=True)
    all_files = list(tqdm(all_files_iter, desc="[1/3] Keypoint íŒŒì¼ ì°¾ëŠ” ì¤‘"))
    
    if not all_files:
        print(f"-> [âŒ ì˜¤ë¥˜] '{root_dir}' ê²½ë¡œì—ì„œ Keypoint íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    print(f"âœ… ì´ {len(all_files)}ê°œì˜ keypoint íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì´ì œ ê·¸ë£¹í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # [2/3] Keypoint íŒŒì¼ ê·¸ë£¹í™” (ê²Œì´ì§€ í‘œì‹œ)
    gesture_groups = defaultdict(list)
    for file_path in tqdm(sorted(all_files), desc="[2/3] Keypoint íŒŒì¼ ê·¸ë£¹í™”"):
        filename = os.path.basename(file_path)
        base_name = '_'.join(filename.split('_')[:-2])
        gesture_groups[base_name].append(file_path)

    total_groups_written = 0
    with open(output_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['gesture_base_name', 'label', 'file_paths'])

        # [3/3] CSV íŒŒì¼ ìƒì„± (ê²Œì´ì§€ í‘œì‹œ)
        for base_name, file_list in tqdm(gesture_groups.items(), desc=f"[3/3] '{dataset_name}' CSV ìƒì„±"):
            
            label = label_map.get(base_name)
            
            # ë¼ë²¨ ë§µì— ìˆëŠ” (ë‹¨ì¼ ë‹¨ì–´) ë°ì´í„°ë§Œ CSVì— ì“´ë‹¤
            if label:
                paths_str = ";".join(file_list)
                writer.writerow([base_name, label, paths_str])
                total_groups_written += 1
    
    print(f"âœ… ì„±ê³µ! ì´ {len(gesture_groups)}ê°œ keypoint ê·¸ë£¹ ì¤‘ {total_groups_written}ê°œì˜ ìœ íš¨í•œ ë™ì‘ ê·¸ë£¹ê³¼ íŒŒì¼ ê²½ë¡œë¥¼ '{output_filename}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


if __name__ == '__main__':
    for name, path in sets_to_process.items():
        create_index_file_optimized(name, path)