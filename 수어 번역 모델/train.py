import torch
import torch.nn as nn
import torch.optim as optim
# ğŸ’¡ [ìˆ˜ì •] random_split ì„í¬íŠ¸ í™•ì¸
from torch.utils.data import DataLoader, random_split
from torch.optim.lr_scheduler import ReduceLROnPlateau
from tqdm import tqdm
import pandas as pd
import numpy as np

from data_loader import SignLanguageDataset # ğŸ’¡ ìˆ˜ì •ëœ data_loader ì„í¬íŠ¸
from model import LSTMClassifier

# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 64
LEARNING_RATE = 0.001
NUM_EPOCHS = 15       # ğŸ’¡ [ìˆ˜ì •] ì¡°ê¸° ì¢…ë£Œë¥¼ ëŒ€ë¹„í•´ ìµœëŒ€ Epoch ì¦ê°€
MAX_LEN = 30
INPUT_SIZE = 548      # ğŸ’¡ [ìˆ˜ì •] (Pose:50 + Face:140 + Hands:84) * 2 (pos+mot) = 548
HIDDEN_SIZE = 512     # ğŸ’¡ [ìˆ˜ì •] ëª¨ë¸ í‘œí˜„ë ¥ ì¦ê°€
NUM_LAYERS = 3        # ğŸ’¡ [ìˆ˜ì •] ëª¨ë¸ ê¹Šì´ ì¦ê°€
NUM_WORKERS = 0
PATIENCE = 5          # ğŸ’¡ [ìˆ˜ì •] ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ Patience ì„¤ì •

# --- ğŸ’¡ [ìˆ˜ì •] ë°ì´í„° ì¤€ë¹„ ë° ë¶„í•  (ì „ëµ ë³€ê²½) ---
print("ë°ì´í„° ì¸ë±ìŠ¤ ë¡œë”©...")
# 1. í•™ìŠµ/ê²€ì¦ìš© ë°ì´í„° ë¡œë“œ (1.Training í´ë” ê¸°ë°˜)
train_valid_df = pd.read_csv('training_index.csv')
# 2. í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë“œ (2.Validation í´ë” ê¸°ë°˜)
test_df = pd.read_csv('validation_index.csv')

# --- ğŸ’¡ [ìˆ˜ì •] í†µí•© ì‚¬ì „ ìƒì„± (ì¤‘ìš”!) ---
# ëª¨ë¸ì´ Train/Valid/Testì—ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” 'ëª¨ë“ ' ë‹¨ì–´ë¥¼ ì•Œì•„ì•¼ í•˜ë¯€ë¡œ,
# 'ì‚¬ì „'ì„ ë§Œë“¤ ë•Œë§Œ ë‘ ë°ì´í„°ë¥¼ ì„ì‹œë¡œ í•©ì¹©ë‹ˆë‹¤.
print("í†µí•© ë¼ë²¨ ì‚¬ì „ ìƒì„±...")
all_df_for_labels = pd.concat([train_valid_df, test_df], ignore_index=True)
all_labels = sorted(all_df_for_labels['label'].unique())

label_to_idx = {label: i for i, label in enumerate(all_labels)}
idx_to_label = {i: label for i, label in enumerate(all_labels)}
label_maps = {'label_to_idx': label_to_idx, 'idx_to_label': idx_to_label}
num_classes = len(all_labels)
print(f"í†µí•©ëœ ì´ í´ë˜ìŠ¤(ë‹¨ì–´) ìˆ˜: {num_classes}")

# --- ğŸ’¡ [ìˆ˜ì •] ë°ì´í„°ì…‹ ìƒì„± ë° ë¶„í•  (ì „ëµ ë³€ê²½) ---
print("ë°ì´í„°ì…‹ ë¡œë”© ë° ë¶„í•  ì‹œì‘...")

# 1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹(Test Dataset) ìƒì„±
# (2.Validation í´ë” ë°ì´í„° ì‚¬ìš©)
test_dataset = SignLanguageDataset(
    data_frame=test_df,  # ğŸ’¡ [ìˆ˜ì •] test_df ì „ë‹¬
    max_len=MAX_LEN,
    label_maps=label_maps
)
print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹(Test): {len(test_dataset)}ê°œ (ì›ë³¸ 'validation_index.csv' ê¸°ë°˜)")

# 2. í•™ìŠµ/ê²€ì¦ í†µí•© ë°ì´í„°ì…‹(Train/Valid Dataset) ìƒì„±
# (1.Training í´ë” ë°ì´í„° ì‚¬ìš©)
full_train_valid_dataset = SignLanguageDataset(
    data_frame=train_valid_df, # ğŸ’¡ [ìˆ˜ì •] train_valid_df ì „ë‹¬
    max_len=MAX_LEN,
    label_maps=label_maps
)
total_tv_size = len(full_train_valid_dataset)
print(f"ì›ë³¸ í•™ìŠµ/ê²€ì¦ ë°ì´í„° ì´: {total_tv_size}ê°œ (ì›ë³¸ 'training_index.csv' ê¸°ë°˜)")

# 3. í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹ì„ 80% / 20%ë¡œ ë¶„í•  (ì˜ˆì‹œ)
# (1.Training í´ë” ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ ë‚˜ëˆ”)
train_size = int(0.8 * total_tv_size)
valid_size = total_tv_size - train_size

if valid_size == 0 and train_size > 0: # 1ê°œë¼ë„ ê²€ì¦ì…‹ì— í• ë‹¹
    train_size -= 1
    valid_size = 1

print(f"ë¶„í•  ë¹„ìœ¨ (training_index ê¸°ì¤€) -> í•™ìŠµ: {train_size} (80%), ê²€ì¦: {valid_size} (20%)")

train_dataset, valid_dataset = random_split(
    full_train_valid_dataset, 
    [train_size, valid_size],
    generator=torch.Generator().manual_seed(42) # ğŸ’¡ ì¬í˜„ì„±ì„ ìœ„í•´ ì‹œë“œ ê³ ì •
)

# --- ğŸ’¡ [ìˆ˜ì •] ë°ì´í„° ë¡œë” ìƒì„± ---
train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)
valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)
test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)

print(f"ë¶„ë¦¬ëœ í•™ìŠµ(Train) ë°ì´í„° ìˆ˜: {len(train_dataset)}")
print(f"ë¶„ë¦¬ëœ ê²€ì¦(Validation) ë°ì´í„° ìˆ˜: {len(valid_dataset)}")
print(f"ë¶„ë¦¬ëœ í…ŒìŠ¤íŠ¸(Test) ë°ì´í„° ìˆ˜: {len(test_dataset)}")

# --- ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ ì •ì˜ ---
model = LSTMClassifier(
    input_size=INPUT_SIZE,
    hidden_size=HIDDEN_SIZE,
    num_layers=NUM_LAYERS,
    num_classes=num_classes
    # ğŸ’¡ Dropout(0.6)ì€ model.pyì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì ìš©ë¨
).to(DEVICE)

criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)

# ğŸ’¡ [ìˆ˜ì •] ì¡°ê¸° ì¢…ë£Œ ë³€ìˆ˜ ì´ˆê¸°í™”
best_val_loss = np.inf
patience_counter = 0
best_model_path = 'sign_language_model_best.pth'

# --- ëª¨ë¸ í•™ìŠµ ì‹œì‘ ---
print(f"\nëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤... (DEVICE: {DEVICE})")
for epoch in range(NUM_EPOCHS):
    model.train()
    train_loss = 0.0
    
    for keypoints, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Training]"):
        keypoints, labels = keypoints.to(DEVICE), labels.to(DEVICE)
        
        outputs = model(keypoints)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        train_loss += loss.item()

    # --- ê²€ì¦ ---
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for keypoints, labels in tqdm(valid_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Validation]"):
            keypoints, labels = keypoints.to(DEVICE), labels.to(DEVICE)
            
            outputs = model(keypoints)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    avg_train_loss = train_loss / len(train_loader)
    avg_val_loss = val_loss / len(valid_loader)
    accuracy = (100 * correct / total) if total > 0 else 0
    
    print(f"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%")
    
    scheduler.step(avg_val_loss)

    # ğŸ’¡ [ìˆ˜ì •] ì¡°ê¸° ì¢…ë£Œ ë¡œì§
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        patience_counter = 0
        torch.save(model.state_dict(), best_model_path)
        print(f"ğŸ‰ New best model saved with Val Loss: {best_val_loss:.4f}")
    else:
        patience_counter += 1
        print(f"Patience: {patience_counter}/{PATIENCE}")
    
    if patience_counter >= PATIENCE:
        print(f"Early stopping triggered after {epoch+1} epochs.")
        break

print("\ní•™ìŠµ ì™„ë£Œ!")

# --- ìµœì¢… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---
print(f"\nìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸({best_model_path})ì„ ë¡œë“œí•˜ì—¬ ìµœì¢… í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")
# ğŸ’¡ [ìˆ˜ì •] ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ì„ ë¡œë“œ
try:
    model.load_state_dict(torch.load(best_model_path))
except FileNotFoundError:
    print(f"[ê²½ê³ ] {best_model_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ epochì˜ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    
model.eval()
test_correct = 0
test_total = 0
with torch.no_grad():
    for keypoints, labels in tqdm(test_loader, desc="Final Test"):
        keypoints, labels = keypoints.to(DEVICE), labels.to(DEVICE)
        outputs = model(keypoints)
        _, predicted = torch.max(outputs.data, 1)
        test_total += labels.size(0)
        test_correct += (predicted == labels).sum().item()

test_accuracy = (100 * test_correct / test_total) if test_total > 0 else 0
print(f"\nìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Test Accuracy): {test_accuracy:.2f}%")

print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ '{best_model_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")