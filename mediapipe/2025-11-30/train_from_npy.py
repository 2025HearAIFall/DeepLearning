# train_from_npy.py (v4.2: Bi-GRU, Relative Normalization for Accuracy)
# -----------------------------------------------------------------------------
# [ë³€ê²½ ì‚¬í•­]: 
# 1. Bidirectional GRU ë° Attention êµ¬ì¡° ë°˜ì˜ (ì„±ëŠ¥ í•µì‹¬ ê°œì„ )
# 2. Keypoint ì •ê·œí™” (Nose ê¸°ì¤€) ì ìš© (ë°ì´í„° ì¼ê´€ì„± í™•ë³´)
# 3. Dropout í™•ë¥ ì„ 0.7 -> 0.5ë¡œ ì¡°ì •
# -----------------------------------------------------------------------------

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau
from tqdm import tqdm
import pandas as pd
import numpy as np
import random
import pickle
import os 
from collections import Counter

# --- ì„¤ì • ---
TRAIN_INDEX_FILE = 'train_augmented.csv'
VAL_INDEX_FILE   = 'val_npy_index.csv'

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
MAX_LEN = 30
INPUT_SIZE = (33 + 21 + 21) * 2 * 2 
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 64
LEARNING_RATE = 0.0001
NUM_EPOCHS = 100
MAX_TARGET_LEN = 50
HIDDEN_SIZE = 256
NUM_LAYERS = 2
EMBED_SIZE = 256
DROPOUT_PROB = 0.5  # [ìˆ˜ì •] 0.7ì—ì„œ 0.5ë¡œ ë‚®ì¶° ê³¼ì†Œì í•© ë°©ì§€

TEACHER_FORCING_START = 1.0
TEACHER_FORCING_END = 0.0

# --- Tokenizer & Vocab ---
def simple_tokenizer(text):
    return text.split(' ')

class Vocabulary:
    def __init__(self, tokenizer, min_freq=2):
        self.tokenizer = tokenizer
        self.itos = {0: "<PAD>", 1: "<SOS>", 2: "<EOS>", 3: "<UNK>"}
        self.stoi = {v: k for k, v in self.itos.items()}
        self.min_freq = min_freq
        
    def __len__(self): 
        return len(self.itos)

    def build_vocab(self, sentence_list):
        counter = Counter()
        for sentence in sentence_list: counter.update(self.tokenizer(sentence))
        idx = 4 
        for word, freq in counter.items():
            if freq >= self.min_freq: self.stoi[word] = idx; self.itos[idx] = word; idx += 1
            
    def numericalize(self, text):
        tokens = self.tokenizer(text)
        return [self.stoi.get(token, self.stoi["<UNK>"]) for token in tokens]
        
    @property
    def pad_idx(self): return self.stoi["<PAD>"]
    @property
    def sos_idx(self): return self.stoi["<SOS>"]
    @property
    def eos_idx(self): return self.stoi["<EOS>"]
    @property
    def unk_idx(self): return self.stoi["<UNK>"]

# --- Dataset (Normalization & Masking ì ìš©) ---
class SignLanguageDataset_InMemory(Dataset):
    def __init__(self, df, max_target_len=50, vocab=None, augment=False):
        self.max_target_len = max_target_len
        self.vocab = vocab
        self.augment = augment
        self.data_list = []
        
        print(f"Loading {len(df)} npy files into memory...")
        for _, row in tqdm(df.iterrows(), total=len(df)):
            npy_path = row['npy_path']
            sentence = row['sentence']
            try:
                seq = np.load(npy_path)
                self.data_list.append((seq, sentence))
            except Exception:
                pass

    # [ìƒˆë¡œ ì¶”ê°€] ì½”(Nose) ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ì¢Œí‘œë¡œ ë³€í™˜ (ìœ„ì¹˜ ë¬´ê´€ í•™ìŠµ)
    def normalize_pose(self, seq):
        # seq shape: (Length, 300) -> [x1, y1, x2, y2, ..., motion_x1, motion_y1, ...]
        
        # ì›ë³¸ (150D Position) ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ì—¬ ìƒëŒ€ ì¢Œí‘œ ê³„ì‚°
        position_part = seq[:, :150].copy()
        
        # ì½”(Nose)ì˜ x, y ì¢Œí‘œ ì¸ë±ìŠ¤ëŠ” 0, 1
        nose_x = position_part[:, 0].reshape(-1, 1)
        nose_y = position_part[:, 1].reshape(-1, 1)
        
        # ëª¨ë“  x, y ì¢Œí‘œì—ì„œ ì½”ì˜ ì¢Œí‘œë¥¼ ë¹¼ì¤Œ (ìƒëŒ€ ì¢Œí‘œí™”)
        for i in range(0, position_part.shape[1], 2):
            position_part[:, i] -= nose_x.squeeze()
            position_part[:, i+1] -= nose_y.squeeze()
            
        # Motion partëŠ” ì´ë¯¸ ìƒëŒ€ì  ë³€í™”ëŸ‰ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€
        motion_part = seq[:, 150:]
        
        return np.concatenate((position_part, motion_part), axis=1)

    # í”„ë ˆì„ ë§ˆìŠ¤í‚¹
    def apply_frame_masking(self, seq, mask_prob=0.15, max_mask_len=5):
        seq_len = seq.shape[0]
        if seq_len < 10: return seq 
        
        if np.random.rand() < mask_prob:
            t0 = np.random.randint(0, seq_len - max_mask_len)
            # ë§ˆìŠ¤í‚¹ ë¶€ë¶„ 0ìœ¼ë¡œ ì±„ì›€
            seq[t0 : t0 + max_mask_len] = 0 
        return seq
        
    def __len__(self):
        return len(self.data_list)
    
    def __getitem__(self, idx):
        final_sequence, sentence_str = self.data_list[idx]
        
        # 1. ì •ê·œí™” ì ìš© (í•™ìŠµ/ê²€ì¦ ëª¨ë‘ ìœ„ì¹˜ ì •ë³´ ì œê±°)
        final_sequence = self.normalize_pose(final_sequence)
        
        # 2. í•™ìŠµìš© ë°ì´í„°ë©´ ë§ˆìŠ¤í‚¹ ì ìš©
        if self.augment:
            # Maskingì€ ì›ë³¸ ë°ì´í„°ë¥¼ ê±´ë“œë¦¬ì§€ ì•Šë„ë¡ ë³µì‚¬í•´ì„œ ì ìš©
            final_sequence = final_sequence.copy() 
            final_sequence = self.apply_frame_masking(final_sequence)
            
        indices = self.vocab.numericalize(sentence_str)
        indices = [self.vocab.sos_idx] + indices + [self.vocab.eos_idx]
        
        # Padding
        if len(indices) < self.max_target_len:
            indices = indices + [self.vocab.pad_idx] * (self.max_target_len - len(indices))
        else:
            indices = indices[:self.max_target_len]
            
        target_tensor = torch.tensor(indices, dtype=torch.long)
        return torch.from_numpy(final_sequence), target_tensor

# --- Models (Bidirectional GRU Architecture) ---
class Encoder(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, dropout_prob):
        super(Encoder, self).__init__()
        # [ìˆ˜ì •] LSTM -> GRU, bidirectional=True
        self.gru = nn.GRU(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout_prob, bidirectional=True)
        self.dropout = nn.Dropout(dropout_prob)
        # ì–‘ë°©í–¥ hidden stateë¥¼ í•©ì¹˜ê¸° ìœ„í•œ FC layer
        self.fc = nn.Linear(hidden_size * 2, hidden_size) 

    def forward(self, x):
        # outputs: (batch, seq_len, hidden*2)
        # hidden: (num_layers*2, batch, hidden)
        outputs, hidden = self.gru(x)
        
        # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ì–‘ë°©í–¥ hidden stateë¥¼ í•©ì¹˜ê³  tanh ì ìš© (Decoderì˜ ì´ˆê¸° hidden state)
        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))
        # hidden shape: (batch, hidden) -> Decoderì— ë§ê²Œ (1, batch, hidden)ìœ¼ë¡œ ë³€í™˜
        hidden = hidden.unsqueeze(0).repeat(self.gru.num_layers, 1, 1)

        # GRUëŠ” cell stateê°€ ì—†ìŠµë‹ˆë‹¤.
        return outputs, hidden

class Attention(nn.Module):
    def __init__(self, hidden_size):
        super(Attention, self).__init__()
        # [ìˆ˜ì •] Encoderê°€ Bi-GRU(hidden*2)ì´ê³  Decoder hiddenì€ ë‹¨ë°©í–¥(hidden)
        self.attn = nn.Linear(hidden_size * 2 + hidden_size, hidden_size)
        self.v = nn.Linear(hidden_size, 1, bias=False)

    def forward(self, hidden, encoder_outputs):
        # hidden: (num_layers, batch, hidden)
        # last_hidden: (batch, hidden)
        last_hidden = hidden[-1]
        src_len = encoder_outputs.shape[1]
        
        # hiddenì„ sequence lengthë§Œí¼ ë³µì‚¬: (batch, src_len, hidden)
        last_hidden = last_hidden.unsqueeze(1).repeat(1, src_len, 1)
        
        # energy = tanh(W_e * [hidden; encoder_outputs])
        energy = torch.tanh(self.attn(torch.cat((last_hidden, encoder_outputs), dim=2)))
        attention = self.v(energy).squeeze(2)
        return torch.softmax(attention, dim=1).unsqueeze(1)

class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hidden_size, num_layers, dropout_prob):
        super(Decoder, self).__init__()
        self.output_dim = output_dim
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.attention = Attention(hidden_size)
        # [ìˆ˜ì •] LSTM -> GRU, ì…ë ¥ í¬ê¸°ëŠ” Context(hidden*2) + Embedding(emb_dim)
        self.gru = nn.GRU(hidden_size * 2 + emb_dim, hidden_size, num_layers, 
                          batch_first=True, dropout=dropout_prob)
        # ì¶œë ¥ í¬ê¸°: Output(hidden) + Embedded(emb) + Context(hidden*2)
        self.fc_out = nn.Linear(hidden_size + emb_dim + hidden_size * 2, output_dim)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, input_token, hidden, encoder_outputs):
        # input_token: (batch) -> (batch, 1)
        input_token = input_token.unsqueeze(1)
        embedded = self.dropout(self.embedding(input_token)) # (batch, 1, emb_dim)
        
        # Attention ì ìš© (hidden[-1]ì€ last layer hidden state)
        attn_weights = self.attention(hidden, encoder_outputs)
        
        # context: (batch, 1, hidden*2)
        context = torch.bmm(attn_weights, encoder_outputs)
        
        # GRU ì…ë ¥: Embedding + Context
        rnn_input = torch.cat((embedded, context), dim=2)
        
        # Decoder GRU ì‹¤í–‰
        # output: (batch, 1, hidden)
        output, hidden = self.gru(rnn_input, hidden)
        
        # ì˜ˆì¸¡ (Output + Embedded + Context)
        prediction = self.fc_out(torch.cat((output.squeeze(1), embedded.squeeze(1), context.squeeze(1)), dim=1))
        
        return prediction, hidden

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size = trg.shape[0]
        trg_len = trg.shape[1]
        trg_vocab_size = self.decoder.output_dim
        
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)
        
        # [ìˆ˜ì •] EncoderëŠ” hidden stateë§Œ ë°˜í™˜ (GRU)
        encoder_outputs, hidden = self.encoder(src) 
        
        input_token = trg[:, 0] # Start token <SOS>
        
        for t in range(1, trg_len):
            # [ìˆ˜ì •] DecoderëŠ” hidden stateë§Œ ë°›ìŒ
            output, hidden = self.decoder(input_token, hidden, encoder_outputs)
            outputs[:, t, :] = output
            
            top1 = output.argmax(1) 
            use_teacher_forcing = random.random() < teacher_forcing_ratio
            input_token = trg[:, t] if use_teacher_forcing else top1
            
        return outputs

# --- Main Execution ---
if __name__ == '__main__':
    if not os.path.exists(TRAIN_INDEX_FILE):
        print(f"[ì˜¤ë¥˜] Train íŒŒì¼ ì—†ìŒ: {TRAIN_INDEX_FILE}")
        exit()
    if not os.path.exists(VAL_INDEX_FILE):
        print(f"[ì˜¤ë¥˜] Val íŒŒì¼ ì—†ìŒ: {VAL_INDEX_FILE}")
        exit()

    train_df = pd.read_csv(TRAIN_INDEX_FILE)
    val_df = pd.read_csv(VAL_INDEX_FILE)

    print("Vocabulary êµ¬ì¶• (Train ê¸°ì¤€)...")
    vocab = Vocabulary(simple_tokenizer, min_freq=2)
    vocab.build_vocab(train_df['sentence'].tolist())
    
    # Trainì…‹ë§Œ augment=True
    train_dataset = SignLanguageDataset_InMemory(train_df, MAX_TARGET_LEN, vocab, augment=True)
    if len(val_df) > 0:
        val_dataset = SignLanguageDataset_InMemory(val_df, MAX_TARGET_LEN, vocab, augment=False)
    else:
        val_dataset = train_dataset

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    valid_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    print(f"Vocab Size: {len(vocab)}")
    
    encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT_PROB)
    decoder = Decoder(len(vocab), EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT_PROB)
    model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)
    
    # Label Smoothing ì ìš©
    criterion = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx, label_smoothing=0.1)
    
    # Weight Decay ì ìš©
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)

    best_val_loss = float('inf')
    best_model_path = 'model_a_mediapipe_best.pth'

    print(f"\nğŸš€ Improved Training Started (Epochs: {NUM_EPOCHS})")

    for epoch in range(NUM_EPOCHS):
        current_ratio = max(0.0, TEACHER_FORCING_START - (epoch / NUM_EPOCHS))

        model.train()
        train_loss = 0.0
        
        loop = tqdm(train_loader, desc=f"Ep {epoch+1} (TF:{current_ratio:.2f})")
        for k, t in loop:
            k, t = k.to(DEVICE), t.to(DEVICE)
            optimizer.zero_grad()
            out = model(k, t, teacher_forcing_ratio=current_ratio)
            
            loss = criterion(out[:,1:].reshape(-1, out.shape[-1]), t[:,1:].reshape(-1))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            train_loss += loss.item()
            loop.set_postfix(loss=loss.item())

        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for k, t in valid_loader:
                k, t = k.to(DEVICE), t.to(DEVICE)
                # Teacher Forcing 0.0ìœ¼ë¡œ ì„¤ì •
                out = model(k, t, teacher_forcing_ratio=0.0) 
                loss = criterion(out[:,1:].reshape(-1, out.shape[-1]), t[:,1:].reshape(-1))
                val_loss += loss.item()

        avg_train = train_loss / len(train_loader)
        avg_val = val_loss / len(valid_loader)
        
        print(f"   Train Loss: {avg_train:.4f} | Val Loss: {avg_val:.4f}")
        scheduler.step(avg_val)
        
        if avg_val < best_val_loss:
            best_val_loss = avg_val
            torch.save(model.state_dict(), best_model_path)
            print(f"   âœ… Best Model Saved ({best_val_loss:.4f})")

    with open('vocab.pkl', 'wb') as f:
        pickle.dump(vocab, f)
    print("\nì™„ë£Œ.")