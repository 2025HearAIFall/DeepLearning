# test_video_with_json_v3.py (v4 - ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìˆ˜ì •)
# -----------------------------------------------------------------------------
# [ëª©í‘œ]: 'train_from_npy.py' (INPUT_SIZE=300)ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
#
# [ì¹˜ëª…ì  ë²„ê·¸ ìˆ˜ì •]:
# - ê¸´ ë¹„ë””ì˜¤(10ì´ˆ)ì—ì„œ 30í”„ë ˆì„ë§Œ ìƒ˜í”Œë§(np.linspace)í•˜ì—¬
#   'ê°€ë§Œíˆ ì„œ ìˆëŠ”' ë¶€ë¶„ë§Œ ë¶„ì„í•˜ë˜ ì˜¤ë¥˜ë¥¼ ìˆ˜ì •.
# - ì‹¤ì‹œê°„ ì¶”ë¡ ê³¼ ë™ì¼í•˜ê²Œ 30í”„ë ˆì„ 'ìŠ¬ë¼ì´ë”© ìœˆë„ìš°' ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•˜ì—¬
#   ëª¨ë“  êµ¬ê°„(3ì´ˆ~6ì´ˆ ë™ì‘ í¬í•¨)ì„ ì˜ˆì¸¡í•˜ë„ë¡ ìˆ˜ì •.
# -----------------------------------------------------------------------------

import cv2
import mediapipe as mp
import onnxruntime as ort
import numpy as np
import pickle
from collections import deque
import sys
import os
import json 
from pathlib import Path

# --- [ì‚¬ìš©ì ì„¤ì •] ---
VIDEO_FILENAME = "NIA_SL_SEN0142_REAL01_F.mp4" 
GROUND_TRUTH_JSON = "NIA_SL_SEN0142_REAL01_F_morpheme.json" 
# ------------------------


# --- [1. ì„¤ì •ê°’] ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
print(f"[INFO] ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€ ê²½ë¡œ (BASE_DIR): {BASE_DIR}")

VIDEO_FILE_PATH = os.path.join(BASE_DIR, VIDEO_FILENAME)
JSON_FILE_PATH = os.path.join(BASE_DIR, GROUND_TRUTH_JSON)

# --- [ìˆ˜ì •] í•˜ì´í¼íŒŒë¼ë¯¸í„° (INPUT_SIZE = 300) ---
MAX_LEN_A = 30
# (Pose:33 + Hands:42) * 2D = 150
BASE_FEATURES_MEDIAPIPE = (33 + 21 + 21) * 2 # 150
# 150 * 2 (ëª¨ì…˜) = 300
INPUT_SIZE_A = BASE_FEATURES_MEDIAPIPE * 2           # 300
START_TOKEN_IDX = 1 # (SOS)
END_TOKEN_IDX = 2   # (EOS)
MAX_OUTPUT_LEN_A = 50
# -------------------------------------------

# --- [ìˆ˜ì •] ONNX ëª¨ë¸ ê²½ë¡œ ---
# 'model_a_face_excluded_best.pth'ë¥¼ ë³€í™˜í•œ íŒŒì¼
ONNX_ENCODER_PATH = os.path.join(BASE_DIR, 'model_a_v2_encoder.onnx')
ONNX_DECODER_PATH = os.path.join(BASE_DIR, 'model_a_v2_decoder.onnx')
VOCAB_PATH = os.path.join(BASE_DIR, 'vocab.pkl')
# -------------------------------------------

# --- Pickle ì˜¤ë¥˜ ìˆ˜ì •ì„ ìœ„í•´ í´ë˜ìŠ¤ ì •ì˜ ì¶”ê°€ (ëª¨ë¸ A) ---
def simple_tokenizer(text):
    return text.split(' ')

class Vocabulary:
    def __init__(self, tokenizer, min_freq=2):
        self.tokenizer = tokenizer
        self.itos = {0: "<PAD>", 1: "<SOS>", 2: "<EOS>", 3: "<UNK>"}
        self.stoi = {v: k for k, v in self.itos.items()}
        self.min_freq = min_freq
    def __len__(self): return len(self.itos)
    def build_vocab(self, sentence_list): pass
    def numericalize(self, text): pass
    @property
    def pad_idx(self): return self.stoi["<PAD>"]
    @property
    def sos_idx(self): return self.stoi["<SOS>"]
    @property
    def eos_idx(self): return self.stoi["<EOS>"]
    @property
    def unk_idx(self): return self.stoi["<UNK>"]

if '__main__' in sys.modules:
    setattr(sys.modules['__main__'], 'Vocabulary', Vocabulary)
# ----------------------------------------------------

# -----------------------------------------------------------------------------
# ëª¨ë¸ A ë¡œë“œ
# -----------------------------------------------------------------------------
print(f"ëª¨ë¸ A (V3 - ì–¼êµ´ ì œì™¸) ë° Vocab ë¡œë“œ ì¤‘...")
try:
    with open(VOCAB_PATH, 'rb') as f:
        vocab = pickle.load(f)
    print(f"Vocabulary ë¡œë“œ ì™„ë£Œ (í¬ê¸°: {len(vocab)})")
    encoder_session = ort.InferenceSession(ONNX_ENCODER_PATH, providers=['CPUExecutionProvider'])
    decoder_session = ort.InferenceSession(ONNX_DECODER_PATH, providers=['CPUExecutionProvider'])
    print(f"ëª¨ë¸ A (V3 - ONNX) ë¡œë“œ ì™„ë£Œ. (Encoder: {ONNX_ENCODER_PATH})")
except Exception as e:
    print(f"[ì˜¤ë¥˜] ëª¨ë¸ A(V3) ë˜ëŠ” Vocab ë¡œë“œ ì‹¤íŒ¨: {e}"); 
    print(f">>> {ONNX_ENCODER_PATH} íŒŒì¼ì´ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit()

# -----------------------------------------------------------------------------
# ì •ë‹µ ë¼ë²¨ ë¡œë“œ (JSON ì‚¬ìš©)
# -----------------------------------------------------------------------------
def get_ground_truth_from_json(json_path):
    print(f"ì •ë‹µ JSON ë¡œë“œ ì¤‘: {json_path}")
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        sentence = ""
        
        if 'sentence' in data:
            sentence = data['sentence']
        elif 'data' in data and isinstance(data['data'], list):
            morphemes = [item['attributes'][0]['name'] for item in data['data'] if 'attributes' in item and item['attributes']]
            sentence = " ".join(morphemes)
        else:
            raise ValueError("JSONì—ì„œ 'sentence' í‚¤ ë˜ëŠ” 'data' ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if not sentence:
             raise ValueError("JSONì€ ì°¾ì•˜ìœ¼ë‚˜ ë¬¸ì¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        print(f"âœ… ì •ë‹µ ë¬¸ì¥ ì°¾ìŒ: {sentence}")
        return sentence
        
    except FileNotFoundError:
        print(f"[ê²½ê³ ] ì •ë‹µ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
        return "N/A (JSON ì—†ìŒ)"
    except Exception as e:
        print(f"[ê²½ê³ ] ì •ë‹µ JSON ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return "N/A (JSON ì˜¤ë¥˜)"

# -----------------------------------------------------------------------------

print("MediaPipe Holistic ëª¨ë¸ ë¡œë“œ ì¤‘...")
mp_holistic = mp.solutions.holistic
holistic = mp_holistic.Holistic(
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# -----------------------------------------------------------------------------
# [ìˆ˜ì •] íŒŒì´í”„ë¼ì¸ A ìœ í‹¸ë¦¬í‹° (ì–¼êµ´ íŠ¹ì§• ì œì™¸)
# -----------------------------------------------------------------------------
def _extract_keypoints_from_frame(frame, holistic) -> np.ndarray:
    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    image.flags.writeable = False
    results = holistic.process(image)
    image.flags.writeable = True
    pose = np.zeros(33 * 2, dtype=np.float32)
    if results.pose_landmarks:
        for i, lm in enumerate(results.pose_landmarks.landmark):
            pose[i*2] = lm.x; pose[i*2 + 1] = lm.y

    # [ìˆ˜ì •] ì–¼êµ´(Face) íŠ¹ì§•ì€ ì¶”ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (train_from_npy.pyì™€ ì¼ì¹˜)
    # face = np.zeros(468 * 2, dtype=np.float32) ...

    lh = np.zeros(21 * 2, dtype=np.float32)
    if results.left_hand_landmarks:
        for i, lm in enumerate(results.left_hand_landmarks.landmark):
            lh[i*2] = lm.x; lh[i*2 + 1] = lm.y
    rh = np.zeros(21 * 2, dtype=np.float32)
    if results.right_hand_landmarks:
        for i, lm in enumerate(results.right_hand_landmarks.landmark):
            rh[i*2] = lm.x; rh[i*2 + 1] = lm.y
            
    # [ìˆ˜ì •] ë°˜í™˜ê°’ì—ì„œ faceë¥¼ ì œì™¸í•©ë‹ˆë‹¤.
    return np.concatenate([pose, lh, rh]) # (150,)

def onnx_predict_realtime(encoder_sess, decoder_sess, src_seq_np):
    try:
        # [í™•ì¸] src_seq_npëŠ” (1, 30, 300)ì´ì–´ì•¼ í•¨
        encoder_inputs = {'input_keypoints': src_seq_np}
        encoder_outputs, hidden, cell = encoder_sess.run(None, encoder_inputs)
        trg_input = np.array([START_TOKEN_IDX], dtype=np.int64) 
        output_tokens = []
        for _ in range(MAX_OUTPUT_LEN_A):
            decoder_inputs = {
                'input_token': trg_input, 'in_hidden': hidden, 
                'in_cell': cell, 'encoder_outputs': encoder_outputs
            }
            logits, hidden, cell = decoder_sess.run(None, decoder_inputs)
            top1_idx_array = np.argmax(logits, axis=1)
            top1_item = top1_idx_array[0]
            if top1_item == END_TOKEN_IDX: break
            output_tokens.append(top1_item)
            trg_input = np.array([top1_item], dtype=np.int64)
        return output_tokens
    except Exception as e:
        print(f"[ì˜¤ë¥˜] ONNX predict ì¤‘ ì˜¤ë¥˜: {e}"); 
        if 'src_seq_np' in locals():
            print(f"  -> ì‹¤ì œ ì…ë ¥ëœ í…ì„œ í¬ê¸°: {src_seq_np.shape}")
        input_name = encoder_sess.get_inputs()[0].name
        expected_shape = encoder_sess.get_inputs()[0].shape
        print(f"  -> ONNX ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ ì´ë¦„: '{input_name}'")
        print(f"  -> ONNX ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ í¬ê¸°: {expected_shape} (batch, 30, 300 ì´ì–´ì•¼ í•¨)")
        return []

# -----------------------------------------------------------------------------
# [ìˆ˜ì •] íŒŒì´í”„ë¼ì¸ A (ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ë³€ê²½)
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# [ìˆ˜ì •] íŒŒì´í”„ë¼ì¸ A (í•™ìŠµ ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ 'ì „ì²´ ìƒ˜í”Œë§ 1íšŒ' ì˜ˆì¸¡)
# -----------------------------------------------------------------------------
def run_whole_video_inference_A(GROUND_TRUTH_SENTENCE):
    print("\n" + "="*30)
    print(f"--- ğŸš€ íŒŒì´í”„ë¼ì¸ A (ë¹„ë””ì˜¤ ì „ì²´ ìƒ˜í”Œë§ í›„ 1íšŒ ì˜ˆì¸¡) ì‹œì‘ ---")
    print(f"íŒŒì¼: {VIDEO_FILE_PATH}")
    print("="*30)

    if not os.path.exists(VIDEO_FILE_PATH):
        print(f"[ì˜¤ë¥˜] ë™ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {VIDEO_FILE_PATH}")
        return

    cap = cv2.VideoCapture(VIDEO_FILE_PATH)
    
    # [ìˆ˜ì •] í(deque) ëŒ€ì‹ , ëª¨ë“  í”„ë ˆì„ê³¼ í‚¤í¬ì¸íŠ¸ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    all_display_frames = []
    all_keypoints_list = []

    print("--- 1ë‹¨ê³„: ë™ì˜ìƒ ì „ì²´ í”„ë ˆì„ 'ìˆ˜ì§‘' ì¤‘... ---")
    frame_count = 0
    while cap.isOpened():
        success, frame = cap.read()
        if not success:
            break
        frame_count += 1
        all_display_frames.append(frame.copy()) 
        
        # 1. í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ í›„ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        keypoints = _extract_keypoints_from_frame(frame, holistic)
        all_keypoints_list.append(keypoints) # (150,)

    cap.release()
    holistic.close()
    
    if not all_keypoints_list:
        print("[ê²½ê³ ] ë¶„ì„ëœ í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    print(f"--- 1ë‹¨ê³„: ìˆ˜ì§‘ ì™„ë£Œ (ì´ {frame_count} í”„ë ˆì„) ---")


    # --- [ìˆ˜ì •] 2ë‹¨ê³„: 'ë‹¨ í•œ ë²ˆ'ì˜ ì˜ˆì¸¡ì„ ìœ„í•œ 30í”„ë ˆì„ ìƒ˜í”Œë§ ---
    print(f"--- 2ë‹¨ê³„: {frame_count}ê°œ í”„ë ˆì„ì„ 30ê°œë¡œ ìƒ˜í”Œë§í•˜ì—¬ 'ë‹¨ 1íšŒ' ì˜ˆì¸¡... ---")
    
    num_frames = len(all_keypoints_list)
    sampled_features = []

    if num_frames > MAX_LEN_A:
        # [ì¤‘ìš”] train_mediapipe.pyì˜ ìƒ˜í”Œë§ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ
        # (ì˜ˆ: 100í”„ë ˆì„ -> 30í”„ë ˆì„ìœ¼ë¡œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§)
        indices = np.linspace(0, num_frames - 1, MAX_LEN_A, dtype=int)
        sampled_features = [all_keypoints_list[i] for i in indices]
    else:
        # 30í”„ë ˆì„ë³´ë‹¤ ì§§ìœ¼ë©´ ê·¸ëƒ¥ ì‚¬ìš©
        sampled_features = all_keypoints_list
        
    # (Keypoints 150D -> Input 300D)
    sequence = np.array(sampled_features, dtype=np.float32) # (<=30, 150)
    
    # íŒ¨ë”© (30í”„ë ˆì„ë³´ë‹¤ ì§§ì€ ë¹„ë””ì˜¤ ëŒ€ì‘)
    if sequence.shape[0] < MAX_LEN_A:
        padding_shape = (MAX_LEN_A - sequence.shape[0], sequence.shape[1])
        padding = np.zeros(padding_shape, dtype=np.float32)
        sequence = np.vstack([sequence, padding]) # (30, 150)

    # ëª¨ì…˜ ë²¡í„° ìƒì„±
    positions = sequence # (30, 150)
    motions = np.zeros_like(positions) # (30, 150)
    if len(positions) > 1:
        motions[1:] = positions[1:] - positions[:-1]
    
    final_sequence = np.concatenate([positions, motions], axis=1)  # (30, 300)
    final_sequence_batch = np.expand_dims(final_sequence, axis=0) # (1, 30, 300)

    # [ì¤‘ìš”] ë¹„ë””ì˜¤ ì „ì²´ì— ëŒ€í•´ 'ë‹¨ 1íšŒ' ì˜ˆì¸¡ ìˆ˜í–‰
    predicted_indices = onnx_predict_realtime(
        encoder_session, decoder_session, final_sequence_batch
    )
    raw_tokens = [vocab.itos.get(idx, "<UNK>") for idx in predicted_indices]
    
    # ì´ê²ƒì´ ìµœì¢… 1íšŒ ì˜ˆì¸¡ ê²°ê³¼ì…ë‹ˆë‹¤.
    final_prediction_one_shot = " ".join(raw_tokens)
    
    print(f"--- [ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ (1-Shot)]: {final_prediction_one_shot} ---")


    # --- [ìˆ˜ì •] 3ë‹¨ê³„: ì¬ìƒ (ì •ë‹µ vs ìµœì¢… ì˜ˆì¸¡ 1ê°œ) ---
    print("--- 3ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ ì¬ìƒ ì‹œì‘... ('q' í‚¤ë¡œ ì¢…ë£Œ) ---")
    
    color_correct = (0, 255, 0) # ì´ˆë¡ìƒ‰ (ì •ë‹µ)
    color_model = (0, 255, 255) # ë…¸ë€ìƒ‰ (ëª¨ë¸)
    font_size = 1.0
    font_thickness = 2
    
    # 0ë²ˆ í”„ë ˆì„ë¶€í„° ì¬ìƒ
    for i, frame in enumerate(all_display_frames):
        
        # [ìˆ˜ì •] ë§¤ë²ˆ ë°”ë€ŒëŠ” ì˜ˆì¸¡ê°’ì´ ì•„ë‹Œ, ìœ„ì—ì„œ í™•ì •ëœ 'ìµœì¢… ì˜ˆì¸¡' ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        current_sentence = final_prediction_one_shot
        
        # 1. ì •ë‹µ í‘œì‹œ
        cv2.putText(frame, 
                    f"Correct: {GROUND_TRUTH_SENTENCE}", 
                    (20, 40), 
                    cv2.FONT_HERSHEY_SIMPLEX, 
                    font_size, 
                    color_correct, 
                    font_thickness)
        
        # 2. ëª¨ë¸ ì˜ˆì¸¡ í‘œì‹œ (ê³ ì •ëœ ìµœì¢… ê²°ê³¼)
        cv2.putText(frame, 
                    f"Model: {current_sentence}", 
                    (20, 90), 
                    cv2.FONT_HERSHEY_SIMPLEX, 
                    font_size, 
                    color_model, 
                    font_thickness)
        
        cv2.imshow('Video File Translation (Ground Truth vs Model)', frame)

        if cv2.waitKey(33) & 0xFF == ord('q'): # 30fps ì†ë„
            break

    cv2.destroyAllWindows()
    print("--- 3ë‹¨ê³„: ì¬ìƒ ì™„ë£Œ ---")

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# =============================================================================
if __name__ == "__main__":
    
    print("\n" + "="*50)
    print("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    print("="*50)
    
    video_path_abs = os.path.abspath(VIDEO_FILE_PATH)
    json_path_abs = os.path.abspath(JSON_FILE_PATH)
    
    GROUND_TRUTH_SENTENCE = get_ground_truth_from_json(json_path_abs)
    
    if os.path.exists(video_path_abs):
        run_whole_video_inference_A(GROUND_TRUTH_SENTENCE)
    else:
        print(f"[ì˜¤ë¥˜] ë©”ì¸ ì‹¤í–‰: ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path_abs}")

    print("\nëª¨ë“  íŒŒì´í”„ë¼ì¸ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
