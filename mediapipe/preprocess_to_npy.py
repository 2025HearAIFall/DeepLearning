# preprocess_to_npy.py (v3.1, ë¡œê·¸ ì œê±° ë° ìµœì í™”)
# -----------------------------------------------------------------------------
# [ìˆ˜ì • ì‚¬í•­]
# 1. ì‹œë„ëŸ¬ìš´ TensorFlow/MediaPipe ê²½ê³  ë¡œê·¸ë¥¼ ìˆ¨ê²¼ìŠµë‹ˆë‹¤.
# 2. ì‘ì—… ì§„í–‰ ì¤‘ ì—ëŸ¬ê°€ ë‚˜ë„ ë©ˆì¶”ì§€ ì•Šê³  ë‹¤ìŒ íŒŒì¼ë¡œ ë„˜ì–´ê°€ë„ë¡ ì˜ˆì™¸ì²˜ë¦¬ë¥¼ ê°•í™”í–ˆìŠµë‹ˆë‹¤.
# -----------------------------------------------------------------------------

import os
# [ì¤‘ìš”] TensorFlow/MediaPipe ë¡œê·¸ ìˆ¨ê¸°ê¸° ì„¤ì • (import ì „ì— í•´ì•¼ í•¨)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3" 

import sys
import logging

# absl ë¡œê·¸ ìˆ¨ê¸°ê¸° (MediaPipe ë‚´ë¶€ ë¡œê¹…)
logging.getLogger('absl').setLevel(logging.ERROR)

import numpy as np
import pandas as pd
from tqdm import tqdm
from pathlib import Path

try:
    import cv2
    import mediapipe as mp
    mp_holistic = mp.solutions.holistic
except ImportError:
    print("[ì˜¤ë¥˜] mediapipe/opencv ì„¤ì¹˜ í•„ìš”")
    exit()

# --- [ì„¤ì •] ---
MAX_LEN = 30
BASE_FEATURES_MEDIAPIPE = (33 + 21 + 21) * 2 # 150 (Face ì œì™¸)
INPUT_SIZE = BASE_FEATURES_MEDIAPIPE * 2     # 300

# [ì‘ì—… ëª©ë¡ ì •ì˜]
TASKS = [
    ("train_dataset.csv", "npy_train", "train_npy_index.csv"),
    ("val_dataset.csv",   "npy_val",   "val_npy_index.csv")
]

# -------------------------

def _extract_keypoints_from_frame(frame, holistic) -> np.ndarray:
    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    image.flags.writeable = False
    results = holistic.process(image)
    image.flags.writeable = True
    
    # 1. Raw ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ê³¼ ë™ì¼)
    pose = np.zeros(33 * 2, dtype=np.float32)
    if results.pose_landmarks:
        for i, lm in enumerate(results.pose_landmarks.landmark):
            pose[i*2] = lm.x
            pose[i*2 + 1] = lm.y
            
    lh = np.zeros(21 * 2, dtype=np.float32)
    if results.left_hand_landmarks:
        for i, lm in enumerate(results.left_hand_landmarks.landmark):
            lh[i*2] = lm.x
            lh[i*2 + 1] = lm.y
            
    rh = np.zeros(21 * 2, dtype=np.float32)
    if results.right_hand_landmarks:
        for i, lm in enumerate(results.right_hand_landmarks.landmark):
            rh[i*2] = lm.x
            rh[i*2 + 1] = lm.y
            
    # --- [ğŸ”¥ í•µì‹¬ ì¶”ê°€: ì¤‘ì‹¬ì  ê¸°ì¤€ ìƒëŒ€ ì¢Œí‘œ ë³€í™˜] ---
    # MediaPipe Pose 23: Left Hip, 24: Right Hip
    # ì¸ë±ìŠ¤ ê³„ì‚°: 23*2=46, 24*2=48
    lx, ly = pose[46], pose[47]
    rx, ry = pose[48], pose[49]
    
    # ê³¨ë°˜ì´ ê°ì§€ë˜ì—ˆë‹¤ë©´ ê·¸ ì¤‘ì‹¬ì„ (0,0)ìœ¼ë¡œ ì¡ìŒ
    if lx > 0 and rx > 0: 
        cx, cy = (lx + rx) / 2, (ly + ry) / 2
    else:
        cx, cy = 0.5, 0.5 # ê°ì§€ ì•ˆ ë˜ë©´ í™”ë©´ ì¤‘ì•™ ê¸°ì¤€
        
    # ì „ì²´ í‚¤í¬ì¸íŠ¸ í†µí•©
    kps = np.concatenate([pose, lh, rh]) # (150,)
    
    # (x, y) ìŒìœ¼ë¡œ ë¬¶ì–´ì„œ ì¼ê´„ ë¹¼ê¸°
    kps_reshaped = kps.reshape(-1, 2)
    kps_reshaped[:, 0] -= cx
    kps_reshaped[:, 1] -= cy
    
    # ë‹¤ì‹œ 1ì°¨ì›ìœ¼ë¡œ í’€ê¸° (í´ë¦¬í•‘ ë²”ìœ„ ì¡°ì •: -1.0 ~ 1.0)
    kps = kps_reshaped.flatten()
    kps = np.clip(kps, -1.0, 1.0) # ìƒëŒ€ ì¢Œí‘œì´ë¯€ë¡œ ìŒìˆ˜ ê°€ëŠ¥
    
    return kps.astype(np.float32)

def process_video_to_npy(video_path: str, holistic, max_len: int) -> np.ndarray:
    cap = cv2.VideoCapture(video_path)
    frames_features = []
    
    if not cap.isOpened():
        return np.zeros((max_len, INPUT_SIZE), dtype=np.float32)
        
    while cap.isOpened():
        success, frame = cap.read()
        if not success: break
        
        # í”„ë ˆì„ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ë°©ì§€
        if frame is None: continue

        keypoints = _extract_keypoints_from_frame(frame, holistic)
        frames_features.append(keypoints)
        
    cap.release()

    if not frames_features:
        return np.zeros((max_len, INPUT_SIZE), dtype=np.float32)

    # ìƒ˜í”Œë§ (í”„ë ˆì„ ìˆ˜ ë§ì¶”ê¸°)
    num_frames = len(frames_features)
    if num_frames > max_len:
        indices = np.linspace(0, num_frames - 1, max_len, dtype=int)
        sampled = [frames_features[i] for i in indices]
    else:
        sampled = frames_features

    seq = np.array(sampled, dtype=np.float32)
    
    # Motion feature (í˜„ì¬ í”„ë ˆì„ - ì´ì „ í”„ë ˆì„)
    motions = np.zeros_like(seq)
    if len(seq) > 1:
        motions[1:] = seq[1:] - seq[:-1]
    
    final = np.concatenate([seq, motions], axis=1)

    # Padding (ëª¨ìë€ í”„ë ˆì„ 0ìœ¼ë¡œ ì±„ìš°ê¸°)
    if final.shape[0] < max_len:
        pad = np.zeros((max_len - final.shape[0], final.shape[1]), dtype=np.float32)
        final = np.vstack([final, pad])

    return final

def process_dataset_task(in_csv, save_dir, out_csv, holistic):
    print(f"\n>>> ì‘ì—… ì‹œì‘: {in_csv} -> {save_dir}")
    
    if not os.path.exists(in_csv):
        print(f"[ìŠ¤í‚µ] ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {in_csv}")
        return

    os.makedirs(save_dir, exist_ok=True)
    df = pd.read_csv(in_csv)
    
    new_rows = []
    
    # tqdm ì„¤ì • (ë¡œê·¸ ë•Œë¬¸ì— ì¤„ë°”ê¿ˆ ë˜ëŠ” í˜„ìƒ ë°©ì§€ìš© leave=True)
    pbar = tqdm(df.iterrows(), total=len(df), desc=f"Processing", unit="video")
    
    for idx, row in pbar:
        video_path = row['video_path']
        sentence = row['sentence']
        
        # íŒŒì¼ëª…ì„ ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ (0.npy, 1.npy...)
        npy_filename = f"{idx}.npy"
        npy_rel_path = os.path.join(save_dir, npy_filename)
        npy_abs_path = os.path.abspath(npy_rel_path)
        
        # ì´ë¯¸ ìƒì„±ëœ íŒŒì¼ì€ ê±´ë„ˆë›°ê¸° (ì´ì–´í•˜ê¸° ê¸°ëŠ¥)
        if os.path.exists(npy_abs_path):
             new_rows.append({
                "npy_path": str(Path(npy_rel_path)).replace('\\', '/'),
                "sentence": sentence
            })
             continue

        try:
            tensor = process_video_to_npy(video_path, holistic, MAX_LEN)
            np.save(npy_abs_path, tensor)
            
            new_rows.append({
                "npy_path": str(Path(npy_rel_path)).replace('\\', '/'),
                "sentence": sentence
            })
        except Exception as e:
            # ì—ëŸ¬ê°€ ë‚˜ë”ë¼ë„ ë©ˆì¶”ì§€ ì•Šê³  ë¡œê·¸ë§Œ ë‚¨ê¹€
            # pbar.writeë¥¼ ì¨ì•¼ ì§„í–‰ë°”ê°€ ê¹¨ì§€ì§€ ì•ŠìŒ
            pbar.write(f"[Error] {video_path}: {e}")
            continue
        
    # ê²°ê³¼ CSV ì €ì¥
    if new_rows:
        pd.DataFrame(new_rows).to_csv(out_csv, index=False, encoding='utf-8-sig')
        print(f"[ì™„ë£Œ] {out_csv} ìƒì„±ë¨ ({len(new_rows)}ê±´)")
    else:
        print(f"[ì£¼ì˜] {out_csv}ì— ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def main():
    # Holistic ëª¨ë¸ ë¡œë“œ
    # static_image_mode=False (ë™ì˜ìƒ ëª¨ë“œ, ë” ë¹ ë¥´ê³  ë¶€ë“œëŸ¬ì›€)
    # model_complexity=1 (ê¸°ë³¸ê°’, ì†ë„ì™€ ì •í™•ë„ ê· í˜•)
    holistic = mp_holistic.Holistic(
        static_image_mode=False, 
        model_complexity=1,
        min_detection_confidence=0.5, 
        min_tracking_confidence=0.5
    )
    
    for input_csv, npy_dir, output_csv in TASKS:
        process_dataset_task(input_csv, npy_dir, output_csv, holistic)
        
    holistic.close()
    print("\nëª¨ë“  ì‘ì—… ì™„ë£Œ.")

if __name__ == "__main__":
    main()