# realtime_v2_integrated.py
# -----------------------------------------------------------------------------
# [í†µí•© ìŠ¤í¬ë¦½íŠ¸ - ì‹¤ì‹œê°„ ì „ìš©]
#
# [ê¸°ì¤€]: 'realtime_inference.py' (v2 ëª¨ë¸ ë¡œë“œ)
# [í†µí•©]: 'inference_mediapipe.py'ì˜ ëª¨ë¸ B (ë¬¸ë§¥ ë³µì›) ê¸°ëŠ¥
# [ë³€ê²½]: .csv íŒŒì¼ì´ í•„ìš”í•œ íŒŒì¼ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (1ë²ˆ)ë¥¼ ì™„ì „íˆ ì œê±°í•˜ê³ 
#         ì‹¤í–‰ ì¦‰ì‹œ ì‹¤ì‹œê°„ ì›¹ìº  ì¶”ë¡ (ëª¨ë¸ A+B)ë§Œ ì‹œì‘í•˜ë„ë¡ ìˆ˜ì •.
# -----------------------------------------------------------------------------

import cv2
import mediapipe as mp
import onnxruntime as ort
import numpy as np
import pickle
from collections import deque
import sys 

# --- [í†µí•©] ëª¨ë¸ B (ë¬¸ë§¥ ë³µì›)ì„ ìœ„í•œ ì„í¬íŠ¸ ---
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch 

# --- [1. ì„¤ì •ê°’] ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# (ëª¨ë¸ A: ìˆ˜ì–´) --- [MediaPipe ê¸°ì¤€] ---
MAX_LEN = 30
BASE_FEATURES_MEDIAPIPE = (33 + 468 + 21 + 21) * 2 # 1086
INPUT_SIZE = BASE_FEATURES_MEDIAPIPE * 2           # 2172

# --- [ìˆ˜ì •] v2 ëª¨ë¸ ê²½ë¡œë¡œ ë³€ê²½ ---
ONNX_ENCODER_PATH = 'model_a_v2_encoder.onnx'
ONNX_DECODER_PATH = 'model_a_v2_decoder.onnx'
VOCAB_PATH = 'vocab.pkl'

# (ëª¨ë¸ B: ë¬¸ë§¥ ë³µì›)
MODEL_B_PATH_OR_NAME = "."

START_TOKEN_IDX = 1 # (SOS)
END_TOKEN_IDX = 2   # (EOS)
MAX_OUTPUT_LEN = 50 

# --- Pickle ì˜¤ë¥˜ ìˆ˜ì •ì„ ìœ„í•´ í´ë˜ìŠ¤ ì •ì˜ ì¶”ê°€ ---
# (train_mediapipe.pyì—ì„œ ë³µì‚¬)
def simple_tokenizer(text):
    return text.split(' ')

class Vocabulary:
    def __init__(self, tokenizer, min_freq=2):
        self.tokenizer = tokenizer
        self.itos = {0: "<PAD>", 1: "<SOS>", 2: "<EOS>", 3: "<UNK>"}
        self.stoi = {v: k for k, v in self.itos.items()}
        self.min_freq = min_freq
    def __len__(self): return len(self.itos)
    def build_vocab(self, sentence_list): pass
    def numericalize(self, text): pass
    @property
    def pad_idx(self): return self.stoi["<PAD>"]
    @property
    def sos_idx(self): return self.stoi["<SOS>"]
    @property
    def eos_idx(self): return self.stoi["<EOS>"]
    @property
    def unk_idx(self): return self.stoi["<UNK>"]

# [ì¤‘ìš”] Pickle 'AttributeError' í•´ê²°
if '__main__' in sys.modules:
    setattr(sys.modules['__main__'], 'Vocabulary', Vocabulary)
# ----------------------------------------------------


# --- [2. ONNX ëª¨ë¸ ë° Vocab ë¡œë“œ] ---
print("ëª¨ë¸ A (V2-ONNX) ë° Vocab ë¡œë“œ ì¤‘...")
try:
    with open(VOCAB_PATH, 'rb') as f:
        vocab = pickle.load(f)
    print(f"Vocabulary ë¡œë“œ ì™„ë£Œ (í¬ê¸°: {len(vocab)})")
    
    # [ìˆ˜ì •] V2 ê²½ë¡œ ë³€ìˆ˜ ì‚¬ìš©
    encoder_session = ort.InferenceSession(ONNX_ENCODER_PATH, providers=['CPUExecutionProvider'])
    decoder_session = ort.InferenceSession(ONNX_DECODER_PATH, providers=['CPUExecutionProvider'])
    print("ëª¨ë¸ A (V2-ONNX) ë¡œë“œ ì™„ë£Œ.")
    
except Exception as e:
    print(f"[ì˜¤ë¥˜] ëª¨ë¸ A ë˜ëŠ” Vocab ë¡œë“œ ì‹¤íŒ¨: {e}")
    print(f">>> {ONNX_ENCODER_PATH}, {ONNX_DECODER_PATH}, {VOCAB_PATH} íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    encoder_session, decoder_session, vocab = None, None, None
    exit() # ëª¨ë¸ Aê°€ ì—†ìœ¼ë©´ ì‹¤í–‰ ë¶ˆê°€

# --- [í†µí•©] ëª¨ë¸ B (ë¬¸ë§¥ ë³µì›) ë¡œë“œ ---
print("ëª¨ë¸ B (ë¬¸ë§¥ ë³µì›) ë¡œë”© ì¤‘...")
try:
    gec_tokenizer = AutoTokenizer.from_pretrained(MODEL_B_PATH_OR_NAME)
    gec_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_B_PATH_OR_NAME).to(DEVICE)
    print("ëª¨ë¸ B ë¡œë“œ ì™„ë£Œ.")
except Exception as e:
    print(f"[ì˜¤ë¥˜] ëª¨ë¸ B ë¡œë“œ ì‹¤íŒ¨: {e}")
    print(">>> ëª¨ë¸ B (Hugging Face) íŒŒì¼ë“¤ì´ '.' ê²½ë¡œì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    gec_model = None; gec_tokenizer = None
    exit() # ëª¨ë¸ Bê°€ ì—†ìœ¼ë©´ í†µí•© ì˜ë¯¸ê°€ ì—†ìœ¼ë¯€ë¡œ ì¢…ë£Œ (ë˜ëŠ” ì´ exit()ë¥¼ ì œê±°í•˜ê³  ì›ë³¸ë§Œ ë³´ë„ë¡ í•´ë„ ë¨)

# --- [3. MediaPipe ëª¨ë“ˆ ë¡œë“œ] ---
print("MediaPipe Holistic ëª¨ë¸ ë¡œë“œ ì¤‘...")
mp_holistic = mp.solutions.holistic
holistic = mp_holistic.Holistic(
    min_detection_confidence=0.5, 
    min_tracking_confidence=0.5
)

# --- [4. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜] ---

def _extract_keypoints_from_frame(frame, holistic) -> np.ndarray:
    """ë‹¨ì¼ í”„ë ˆì„ì—ì„œ 1086ê°œì˜ 2D í‚¤í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    image.flags.writeable = False
    results = holistic.process(image)
    image.flags.writeable = True
    
    pose = np.zeros(33 * 2, dtype=np.float32)
    if results.pose_landmarks:
        for i, lm in enumerate(results.pose_landmarks.landmark):
            pose[i*2] = lm.x; pose[i*2 + 1] = lm.y
    face = np.zeros(468 * 2, dtype=np.float32)
    if results.face_landmarks:
        for i, lm in enumerate(results.face_landmarks.landmark):
            face[i*2] = lm.x; face[i*2 + 1] = lm.y
    lh = np.zeros(21 * 2, dtype=np.float32)
    if results.left_hand_landmarks:
        for i, lm in enumerate(results.left_hand_landmarks.landmark):
            lh[i*2] = lm.x; lh[i*2 + 1] = lm.y
    rh = np.zeros(21 * 2, dtype=np.float32)
    if results.right_hand_landmarks:
        for i, lm in enumerate(results.right_hand_landmarks.landmark):
            rh[i*2] = lm.x; rh[i*2 + 1] = lm.y
    return np.concatenate([pose, face, lh, rh]) # (1086,)

def onnx_predict_realtime(encoder_sess, decoder_sess, src_seq_np):
    """V2(ë‹¨ì¼ ë·°) ONNX ëª¨ë¸ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤."""
    # src_seq_np: (1, 30, 2172)
    
    try:
        # Encoder ì‹¤í–‰
        encoder_inputs = {'input_keypoints': src_seq_np}
        encoder_outputs, hidden, cell = encoder_sess.run(None, encoder_inputs)
        
        # Decoder ì´ˆê¸° ì…ë ¥ (SOS í† í°)
        trg_input = np.array([[START_TOKEN_IDX]], dtype=np.int64)
        
        output_tokens = []
        for _ in range(MAX_OUTPUT_LEN):
            decoder_inputs = {
                'input_token': trg_input, 
                'in_hidden': hidden, 
                'in_cell': cell, 
                'encoder_outputs': encoder_outputs
            }
            logits, hidden, cell = decoder_sess.run(None, decoder_inputs)
            
            top1_idx_array = np.argmax(logits, axis=1)
            top1_item = top1_idx_array[0]
            
            if top1_item == END_TOKEN_IDX:
                break
                
            output_tokens.append(top1_item)
            trg_input = np.array([[top1_item]], dtype=np.int64)
            
        return output_tokens
    except Exception as e:
        print(f"[ì˜¤ë¥˜] ONNX predict ì¤‘ ì˜¤ë¥˜: {e}")
        return []

# --- [5. ì‹¤ì‹œê°„ ì¶”ë¡  ë£¨í”„ (ëª¨ë¸ B í†µí•©)] ---
def run_realtime_inference():
    print("\n" + "="*30)
    print("--- ğŸš€ ì‹¤ì‹œê°„ ì¶”ë¡  ëª¨ë“œ ì‹œì‘ ---")
    print("="*30)

    print("ì›¹ìº ì„ ì‹œì‘í•©ë‹ˆë‹¤... ('q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤)")
    cap = cv2.VideoCapture(0) # 0ë²ˆ ì¹´ë©”ë¼

    # MAX_LEN (30) í”„ë ˆì„ì˜ í‚¤í¬ì¸íŠ¸ë¥¼ ì €ì¥í•  í
    keypoint_queue = deque(maxlen=MAX_LEN)
    predicted_sentence = ""
    corrected_sentence = "" # [í†µí•©] êµì •ëœ ë¬¸ì¥

    while cap.isOpened():
        success, frame = cap.read()
        if not success:
            print("ì¹´ë©”ë¼ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        # 1. MediaPipe í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        keypoints_1086 = _extract_keypoints_from_frame(frame, holistic)
        
        # 2. íì— í˜„ì¬ í‚¤í¬ì¸íŠ¸ ì¶”ê°€
        keypoint_queue.append(keypoints_1086)

        # 3. ì¶”ë¡  (íê°€ 30í”„ë ˆì„ìœ¼ë¡œ ì°¼ì„ ë•Œë§Œ)
        if len(keypoint_queue) == MAX_LEN:
            
            # (30, 1086) ë°°ì—´ ìƒì„±
            sequence = np.array(keypoint_queue, dtype=np.float32)
            
            # (30, 2172) ë°°ì—´ë¡œ ë³€í™˜ (ëª¨ì…˜ ë²¡í„° ì¶”ê°€)
            positions = sequence
            motions = np.zeros_like(positions)
            if len(positions) > 1:
                motions[1:] = positions[1:] - positions[:-1]
            final_sequence = np.concatenate([positions, motions], axis=1)
            
            # (1, 30, 2172) ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            final_sequence_batch = np.expand_dims(final_sequence, axis=0)

            # 4. ONNX ì¶”ë¡  ì‹¤í–‰ (ëª¨ë¸ A)
            predicted_indices = onnx_predict_realtime(
                encoder_session, decoder_session, final_sequence_batch
            )
            
            # 5. ê²°ê³¼ ë””ì½”ë”© (ëª¨ë¸ A)
            raw_tokens = [vocab.itos.get(idx, "<UNK>") for idx in predicted_indices]
            predicted_sentence = " ".join(raw_tokens)
            
            # [í†µí•©] 5-1. ë¬¸ë§¥ ë³µì› (ëª¨ë¸ B)
            if gec_model and gec_tokenizer and len(raw_tokens) >= 3:
                try:
                    inputs = gec_tokenizer(predicted_sentence, return_tensors="pt").to(DEVICE)
                    outputs = gec_model.generate(
                        **inputs, max_length=128, num_beams=5,
                        repetition_penalty=1.2, no_repeat_ngram_size=2,
                        early_stopping=True
                    )
                    corrected_sentence = gec_tokenizer.decode(outputs[0], skip_special_tokens=True)
                except Exception as e:
                    print(f"[ì˜¤ë¥˜] ëª¨ë¸ B ì‹¤ì‹œê°„ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                    corrected_sentence = predicted_sentence # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ì‚¬ìš©
            else:
                corrected_sentence = predicted_sentence # ëª¨ë¸ì´ ì—†ê±°ë‚˜ ë‹¨ì–´ê°€ ì§§ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
            
            # (ì¶”ë¡  íš¨ìœ¨ì„ ìœ„í•´ íì˜ ì¼ë¶€ë¥¼ ë¹„ì›€ - ì˜ˆ: 5 í”„ë ˆì„)
            for _ in range(5):
                keypoint_queue.popleft()

        # 6. í™”ë©´ì— í‘œì‹œ
        frame = cv2.flip(frame, 1)
        
        status_text = f"Frames: {len(keypoint_queue)}/{MAX_LEN}"
        color = (0, 255, 0) if len(keypoint_queue) == MAX_LEN else (0, 0, 255)
        cv2.putText(frame, status_text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)

        # [ìˆ˜ì •] ì›ë³¸ ë²ˆì—­ê³¼ êµì •ëœ ë²ˆì—­ì„ ëª¨ë‘ í‘œì‹œ
        cv2.putText(frame, f"Raw: {predicted_sentence}", (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)
        cv2.putText(frame, f"Fix: {corrected_sentence}", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)

        cv2.imshow('Real-time Sign Language Translation (Model A + B)', frame)

        if cv2.waitKey(5) & 0xFF == ord('q'):
            break

    # --- [6. ì¢…ë£Œ] ---
    cap.release()
    cv2.destroyAllWindows()
    holistic.close()
    print("ì‹¤ì‹œê°„ ì¶”ë¡ ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

# =============================================================================
# [í†µí•©] ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# =============================================================================
if __name__ == "__main__":
    
    # ëª¨ë¸ Aì™€ Bê°€ ëª¨ë‘ ë¡œë“œë˜ì—ˆëŠ”ì§€ ìµœì¢… í™•ì¸
    if encoder_session and decoder_session and vocab and gec_model and gec_tokenizer:
        print("\n" + "="*50)
        print("âœ… ëª¨ë¸ A (V2-ONNX) ë° ëª¨ë¸ B (ë¬¸ë§¥ ë³µì›) ë¡œë“œ ì™„ë£Œ.")
        print("="*50)
        
        # [ìˆ˜ì •] ë©”ë‰´ ì—†ì´ ì‹¤ì‹œê°„ ì¶”ë¡  ë°”ë¡œ ì‹œì‘
        run_realtime_inference()
        
    else:
        print("\n[ì¹˜ëª…ì  ì˜¤ë¥˜] ëª¨ë¸ A ë˜ëŠ” ëª¨ë¸ B ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ì‹¤ì‹œê°„ ì¶”ë¡ ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ìŠ¤í¬ë¦½íŠ¸ ìƒë‹¨ì˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ê³  í•„ìš”í•œ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        print("ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")