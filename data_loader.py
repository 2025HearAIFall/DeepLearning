# data_loader.py (ìˆ˜ì •: Seq2Seqìš©)

import os
import json
import torch
import numpy as np
import pandas as pd
from torch.utils.data import Dataset
import time

# ğŸ’¡ Vocabulary í´ë˜ìŠ¤ëŠ” train.pyì—ì„œ ì •ì˜í•˜ê³  ì—¬ê¸°ì— ì£¼ì…(inject)í•©ë‹ˆë‹¤.
# (íŒŒì¼ ê°„ ì˜ì¡´ì„±ì„ ì¤„ì´ê¸° ìœ„í•´)

class SignLanguageDataset(Dataset):
    # ğŸ’¡ [ìˆ˜ì •] label_maps ëŒ€ì‹  vocab ê°ì²´ì™€ max_target_len ì¶”ê°€
    def __init__(self, index_file_path, max_len=30, max_target_len=50, vocab=None):
        self.max_len = max_len
        self.max_target_len = max_target_len # ğŸ’¡ íƒ€ê²Ÿ ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´
        self.data_info = pd.read_csv(index_file_path)
        
        # ğŸ’¡ [ìˆ˜ì •] ë¼ë²¨ë§µ ëŒ€ì‹  Vocabulary ê°ì²´ ì‚¬ìš©
        if vocab is None:
            raise ValueError("Vocabulary ê°ì²´ê°€ ì œê³µë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        self.vocab = vocab
    
    def _reshape_and_get_xy(self, keypoints_data, expected_points):
        """í‚¤í¬ì¸íŠ¸ ë°ì´í„°ë¥¼ (N, 3)ìœ¼ë¡œ ë³€í™˜í•˜ê³  x, y ì¢Œí‘œë§Œ ì¶”ì¶œ, ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€"""
        keypoints = np.array(keypoints_data).flatten()
        if keypoints.size == 0:
            return np.zeros((expected_points, 2), dtype=np.float32)
        
        keypoints = keypoints.reshape(-1, 3)
        return keypoints[:, :2]

    def _extract_and_normalize_keypoints(self, file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        person_data = data.get('people', {})

        pose_xy = self._reshape_and_get_xy(
            person_data.get('pose_keypoints_2d', []), 25)
        face_xy = self._reshape_and_get_xy(
            person_data.get('face_keypoints_2d', []), 70)
        left_hand_xy = self._reshape_and_get_xy(
            person_data.get('hand_left_keypoints_2d', []), 21)
        right_hand_xy = self._reshape_and_get_xy(
            person_data.get('hand_right_keypoints_2d', []), 21)

        # 1. ì¤‘ì‹¬ì  ì´ë™ (ëª© 'Neck' ê¸°ì¤€)
        neck = pose_xy[1].copy()
        
        if np.sum(neck**2) > 1e-6:
            pose_xy -= neck
            face_xy -= neck
            left_hand_xy -= neck
            right_hand_xy -= neck
        
        # 2. í¬ê¸° ì •ê·œí™”
        combined_coords = np.vstack([pose_xy, face_xy, left_hand_xy, right_hand_xy])
        max_abs_val = np.max(np.abs(combined_coords))
        
        if max_abs_val > 1e-6:
            pose_xy /= max_abs_val
            face_xy /= max_abs_val
            left_hand_xy /= max_abs_val
            right_hand_xy /= max_abs_val

        # ëª¨ë“  ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ ê²°í•© (274)
        return np.concatenate([
            pose_xy.flatten(), 
            face_xy.flatten(), 
            left_hand_xy.flatten(), 
            right_hand_xy.flatten()
        ])

    def __len__(self):
        return len(self.data_info)

    def __getitem__(self, idx):
        item = self.data_info.iloc[idx]
        
        # ğŸ’¡ [ìˆ˜ì •] 'label' -> 'sentence' (preprocess.pyì™€ ì¼ì¹˜)
        sentence_str = item['sentence']
        
        paths_str = item['file_paths']
        json_paths = paths_str.split(';')
        
        num_frames = len(json_paths)
        if num_frames > self.max_len:
            indices = np.linspace(0, num_frames - 1, self.max_len, dtype=int)
            sampled_paths = [json_paths[i] for i in indices]
        else:
            sampled_paths = json_paths

        sequence = []
        for path in sampled_paths:
            try:
                keypoints = self._extract_and_normalize_keypoints(path)
                sequence.append(keypoints)
            except Exception as e:
                continue
        
        num_features = 274
        
        # ğŸ’¡ [ìˆ˜ì •] íƒ€ê²Ÿ ë¬¸ì¥(label) ì²˜ë¦¬
        # ë¬¸ì¥ì„ í† í° ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (e.g., "ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆë‹¤")
        indices = self.vocab.numericalize(sentence_str)
        
        # <SOS>ì™€ <EOS> í† í° ì¶”ê°€
        indices = [self.vocab.sos_idx] + indices + [self.vocab.eos_idx]
        
        # íƒ€ê²Ÿ ë¬¸ì¥ íŒ¨ë”©
        target_len = len(indices)
        if target_len < self.max_target_len:
            indices.extend([self.vocab.pad_idx] * (self.max_target_len - target_len))
        else:
            indices = indices[:self.max_target_len] # ìë¥´ê¸°
            
        target_tensor = torch.tensor(indices, dtype=torch.long)
        
        # --- í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ---
        if not sequence:
            final_sequence = np.zeros((self.max_len, num_features * 2), dtype=np.float32)
            # ğŸ’¡ [ìˆ˜ì •] (í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤, íƒ€ê²Ÿ ë¬¸ì¥ ì‹œí€€ìŠ¤) ë°˜í™˜
            return torch.from_numpy(final_sequence), target_tensor
            
        sequence = np.array(sequence, dtype=np.float32)
        
        positions = sequence
        motions = np.zeros_like(positions)
        if len(positions) > 1:
            motions[1:] = positions[1:] - positions[:-1]
        
        final_sequence = np.concatenate([positions, motions], axis=1) # (seq_len, 548)

        seq_len = final_sequence.shape[0]
        if seq_len < self.max_len:
            padding = np.zeros((self.max_len - seq_len, final_sequence.shape[1]), dtype=np.float32)
            final_sequence = np.vstack([final_sequence, padding])

        # ğŸ’¡ [ìˆ˜ì •] (í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤, íƒ€ê²Ÿ ë¬¸ì¥ ì‹œí€€ìŠ¤) ë°˜í™˜
        return torch.from_numpy(final_sequence), target_tensor
