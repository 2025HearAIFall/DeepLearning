# model.py (ìˆ˜ì •: Seq2Seq Encoder-Decoder)

import torch
import torch.nn as nn
import random

class Encoder(nn.Module):
    """ ìˆ˜ì–´ í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤. """
    def __init__(self, input_size, hidden_size, num_layers, dropout_prob):
        super(Encoder, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True,
            bidirectional=True, # ğŸ’¡ ì–‘ë°©í–¥ LSTM
            dropout=dropout_prob if num_layers > 1 else 0
        )
        
        self.dropout = nn.Dropout(dropout_prob)
        
        # ğŸ’¡ ì–‘ë°©í–¥ LSTMì˜ hidden/cellì„ ë‹¨ë°©í–¥ Decoderì— ë§ê²Œ ë³€í™˜
        self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)
        self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)

    def forward(self, x):
        # x: (batch_size, seq_len, input_size)
        
        lstm_out, (hidden, cell) = self.lstm(x)
        
        # lstm_out: (batch_size, seq_len, hidden_size * 2)
        # hidden: (num_layers * 2, batch_size, hidden_size)
        # cell: (num_layers * 2, batch_size, hidden_size)
        
        # ğŸ’¡ ì–‘ë°©í–¥ LSTMì˜ ë§ˆì§€ë§‰ hidden/cell stateë¥¼ Decoderì˜ ì´ˆê¸° stateë¡œ ì‚¬ìš©
        # (num_layers * 2, batch_size, hidden_size) -> (num_layers, batch_size, hidden_size * 2)
        hidden = hidden.permute(1, 0, 2).contiguous()
        cell = cell.permute(1, 0, 2).contiguous()

        # (batch_size, num_layers * 2, hidden_size) -> (batch_size, num_layers, hidden_size * 2)
        hidden = hidden.view(hidden.size(0), self.num_layers, self.hidden_size * 2)
        cell = cell.view(cell.size(0), self.num_layers, self.hidden_size * 2)
        
        # (batch_size, num_layers, hidden_size * 2) -> (batch_size, num_layers, hidden_size)
        hidden = torch.tanh(self.fc_hidden(hidden))
        cell = torch.tanh(self.fc_cell(cell))
        
        # (batch_size, num_layers, hidden_size) -> (num_layers, batch_size, hidden_size)
        hidden = hidden.permute(1, 0, 2).contiguous()
        cell = cell.permute(1, 0, 2).contiguous()

        # ğŸ’¡ Decoderë¡œ ì „ë‹¬ë  context vector
        # (num_layers, batch_size, hidden_size)
        return hidden, cell

class Decoder(nn.Module):
    """ Encoderì˜ context vectorë¥¼ ë°›ì•„ ë¬¸ì¥ì„ ë””ì½”ë”©í•©ë‹ˆë‹¤. """
    def __init__(self, output_size, embed_size, hidden_size, num_layers, dropout_prob):
        super(Decoder, self).__init__()
        
        self.output_size = output_size # Target vocab size
        
        self.embedding = nn.Embedding(output_size, embed_size)
        
        self.lstm = nn.LSTM(
            embed_size, 
            hidden_size, 
            num_layers, 
            batch_first=True,
            # ğŸ’¡ DecoderëŠ” ë‹¨ë°©í–¥
            dropout=dropout_prob if num_layers > 1 else 0
        )
        
        self.fc_out = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, x_input, hidden, cell):
        # x_input: (batch_size, 1) - ì´ì „ ìŠ¤í…ì˜ ì˜ˆì¸¡ ë‹¨ì–´
        # hidden, cell: (num_layers, batch_size, hidden_size) - ì´ì „ ìŠ¤í…ì˜ state
        
        # (batch_size, 1) -> (batch_size, 1, embed_size)
        embedded = self.dropout(self.embedding(x_input))
        
        # (batch_size, 1, embed_size) -> (batch_size, 1, hidden_size)
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        
        # (batch_size, 1, hidden_size) -> (batch_size, output_size)
        prediction = self.fc_out(output.squeeze(1))
        
        # (batch_size, output_size), (num_layers, batch_size, hidden_size) * 2
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    """ Encoderì™€ Decoderë¥¼ ê²°í•©í•˜ëŠ” Seq2Seq ëª¨ë¸ """
    def __init__(self, encoder, decoder, device):
        super(Seq2Seq, self).__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src_seq, trg_seq, teacher_forcing_ratio=0.5):
        # src_seq: (batch_size, src_len, input_size)
        # trg_seq: (batch_size, trg_len)
        
        batch_size = trg_seq.shape[0]
        trg_len = trg_seq.shape[1]
        trg_vocab_size = self.decoder.output_size
        
        # Decoder ì¶œë ¥ì„ ì €ì¥í•  í…ì„œ
        # (batch_size, trg_len, trg_vocab_size)
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)
        
        # 1. Encoder
        hidden, cell = self.encoder(src_seq)
        
        # 2. Decoder
        # ì²« ë²ˆì§¸ ì…ë ¥ì€ <SOS> í† í°
        # (batch_size, 1)
        trg_input = trg_seq[:, 0].unsqueeze(1)
        
        for t in range(1, trg_len): # <SOS> ë‹¤ìŒ ë‹¨ì–´ë¶€í„° ì˜ˆì¸¡
            output, hidden, cell = self.decoder(trg_input, hidden, cell)
            
            # (batch_size, trg_vocab_size)
            outputs[:, t, :] = output
            
            # Teacher Forcing: í•™ìŠµ ì‹œ ì‹¤ì œ íƒ€ê²Ÿ ë‹¨ì–´ë¥¼ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
            use_teacher_force = random.random() < teacher_forcing_ratio
            
            # ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´
            top1 = output.argmax(1)
            
            trg_input = (trg_seq[:, t] if use_teacher_force else top1).unsqueeze(1)
            
        return outputs